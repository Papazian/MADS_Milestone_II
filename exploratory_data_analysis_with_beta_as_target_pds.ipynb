{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d8105ff",
   "metadata": {},
   "source": [
    "# Read in the Raw Data, Load the YAML metadata, and Clean the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "232e2424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required modules\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error\n",
    "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
    "from sklearn.metrics.pairwise import pairwise_distances_argmin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e04c7a3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50542, 543)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the raw CSV data, and then print the count of observations and variables\n",
    "# https://www.fhfa.gov/sites/default/files/2024-06/nsmo_v50_1321_puf.csv\n",
    "\n",
    "raw_df = pd.read_csv('nsmo_v50_1321_puf.csv')\n",
    "raw_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9dc6119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YAML files containing metadata into Python as dictionaries\n",
    "\n",
    "# Load the labels of each variable into a dictionary\n",
    "# https://www.fhfa.gov/sites/default/files/2024-06/nsmo_v50_labels.sas\n",
    "\n",
    "with open('variable_labels.yaml', 'r') as file:\n",
    "    variable_labels_dict = yaml.safe_load(file)\n",
    "    \n",
    "# Load the the format of each variable into a dictionary\n",
    "# https://www.fhfa.gov/sites/default/files/2024-06/nsmo_v50_labels.sas\n",
    "\n",
    "with open('variable_formats.yaml', 'r') as file:\n",
    "    variable_formats_dict = yaml.safe_load(file)\n",
    "    \n",
    "# Load the categories for every categorical variable (exclude null categories) into a dictionary\n",
    "# https://www.fhfa.gov/sites/default/files/2024-06/nsmo_v50_formats.sas\n",
    "\n",
    "with open('categorical_variables_categories.yaml', 'r') as file:\n",
    "    categorical_variables_categories_dict = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ca0ba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data by converting negative values and \".\" values (representing missing values) into null values (i.e., NaN)\n",
    "\n",
    "excluded_columns = ['PSTATFM', 'rate_spread']\n",
    "\n",
    "for col in raw_df.columns:\n",
    "    if variable_formats_dict[col] != 'PSTATFM' and col != 'rate_spread':\n",
    "        raw_df.loc[raw_df[col] < 0, col] = np.nan\n",
    "        raw_df.loc[raw_df[col] == \".\", col] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc7beadd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nsmoid</th>\n",
       "      <th>survey_wave</th>\n",
       "      <th>analysis_weight</th>\n",
       "      <th>x05a</th>\n",
       "      <th>x05b</th>\n",
       "      <th>x05c</th>\n",
       "      <th>x05d</th>\n",
       "      <th>x05e</th>\n",
       "      <th>x05f</th>\n",
       "      <th>x05g</th>\n",
       "      <th>...</th>\n",
       "      <th>mtmltv0621</th>\n",
       "      <th>mtmltv0921</th>\n",
       "      <th>mtmltv1221</th>\n",
       "      <th>mtmltv0322</th>\n",
       "      <th>mtmltv0622</th>\n",
       "      <th>mtmltv0922</th>\n",
       "      <th>mtmltv1222</th>\n",
       "      <th>mtmltv0323</th>\n",
       "      <th>mtmltv0623</th>\n",
       "      <th>mtmltv0923</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50537</th>\n",
       "      <td>531289.000</td>\n",
       "      <td>34.000</td>\n",
       "      <td>2117.790</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.000</td>\n",
       "      <td>61.000</td>\n",
       "      <td>59.000</td>\n",
       "      <td>59.000</td>\n",
       "      <td>59.000</td>\n",
       "      <td>59.000</td>\n",
       "      <td>59.000</td>\n",
       "      <td>60.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50538</th>\n",
       "      <td>546643.000</td>\n",
       "      <td>34.000</td>\n",
       "      <td>1738.920</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>79.000</td>\n",
       "      <td>77.000</td>\n",
       "      <td>74.000</td>\n",
       "      <td>72.000</td>\n",
       "      <td>72.000</td>\n",
       "      <td>71.000</td>\n",
       "      <td>71.000</td>\n",
       "      <td>71.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50539</th>\n",
       "      <td>512993.000</td>\n",
       "      <td>34.000</td>\n",
       "      <td>2353.260</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>95.000</td>\n",
       "      <td>91.000</td>\n",
       "      <td>88.000</td>\n",
       "      <td>85.000</td>\n",
       "      <td>84.000</td>\n",
       "      <td>84.000</td>\n",
       "      <td>83.000</td>\n",
       "      <td>82.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50540</th>\n",
       "      <td>518631.000</td>\n",
       "      <td>34.000</td>\n",
       "      <td>5283.750</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.000</td>\n",
       "      <td>53.000</td>\n",
       "      <td>50.000</td>\n",
       "      <td>49.000</td>\n",
       "      <td>49.000</td>\n",
       "      <td>49.000</td>\n",
       "      <td>48.000</td>\n",
       "      <td>48.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50541</th>\n",
       "      <td>544740.000</td>\n",
       "      <td>34.000</td>\n",
       "      <td>1738.920</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.000</td>\n",
       "      <td>74.000</td>\n",
       "      <td>69.000</td>\n",
       "      <td>66.000</td>\n",
       "      <td>65.000</td>\n",
       "      <td>64.000</td>\n",
       "      <td>63.000</td>\n",
       "      <td>63.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 543 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          nsmoid  survey_wave  analysis_weight  x05a  x05b  x05c  x05d  x05e  \\\n",
       "50537 531289.000       34.000         2117.790 1.000 1.000 1.000 1.000 1.000   \n",
       "50538 546643.000       34.000         1738.920 3.000 3.000 2.000 2.000 2.000   \n",
       "50539 512993.000       34.000         2353.260 1.000 2.000 2.000 2.000 2.000   \n",
       "50540 518631.000       34.000         5283.750 3.000 3.000 3.000 3.000 3.000   \n",
       "50541 544740.000       34.000         1738.920 1.000 1.000 1.000 1.000 1.000   \n",
       "\n",
       "       x05f  x05g  ...  mtmltv0621  mtmltv0921  mtmltv1221  mtmltv0322  \\\n",
       "50537 1.000 1.000  ...         NaN         NaN      64.000      61.000   \n",
       "50538 1.000 3.000  ...         NaN         NaN      79.000      77.000   \n",
       "50539 1.000 2.000  ...         NaN         NaN      95.000      91.000   \n",
       "50540 3.000 3.000  ...         NaN         NaN      56.000      53.000   \n",
       "50541 1.000 2.000  ...         NaN         NaN      80.000      74.000   \n",
       "\n",
       "       mtmltv0622  mtmltv0922  mtmltv1222  mtmltv0323  mtmltv0623  mtmltv0923  \n",
       "50537      59.000      59.000      59.000      59.000      59.000      60.000  \n",
       "50538      74.000      72.000      72.000      71.000      71.000      71.000  \n",
       "50539      88.000      85.000      84.000      84.000      83.000      82.000  \n",
       "50540      50.000      49.000      49.000      49.000      48.000      48.000  \n",
       "50541      69.000      66.000      65.000      64.000      63.000      63.000  \n",
       "\n",
       "[5 rows x 543 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out a few obs after data cleaning\n",
    "\n",
    "raw_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1db2d827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of all variable formats\n",
    "\n",
    "variable_formats_set = set(variable_formats_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d39bc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the categorical variables and a list of the numeric variables\n",
    "\n",
    "categorical_variables = []\n",
    "numeric_variables = []\n",
    "\n",
    "categorical_variable_formats = set(categorical_variables_categories_dict.keys())\n",
    "numeric_variable_formats = variable_formats_set - categorical_variable_formats\n",
    "\n",
    "for col in raw_df.columns:\n",
    "    if variable_formats_dict[col] in categorical_variable_formats:\n",
    "        categorical_variables.append(col)\n",
    "    elif variable_formats_dict[col] in numeric_variable_formats:\n",
    "        numeric_variables.append(col)\n",
    "    else:\n",
    "        print(\"Error in bifurcation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06df9139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nsmoid</th>\n",
       "      <th>survey_wave</th>\n",
       "      <th>analysis_weight</th>\n",
       "      <th>x05a</th>\n",
       "      <th>x05b</th>\n",
       "      <th>x05c</th>\n",
       "      <th>x05d</th>\n",
       "      <th>x05e</th>\n",
       "      <th>x05f</th>\n",
       "      <th>x05g</th>\n",
       "      <th>...</th>\n",
       "      <th>mtmltv0621</th>\n",
       "      <th>mtmltv0921</th>\n",
       "      <th>mtmltv1221</th>\n",
       "      <th>mtmltv0322</th>\n",
       "      <th>mtmltv0622</th>\n",
       "      <th>mtmltv0922</th>\n",
       "      <th>mtmltv1222</th>\n",
       "      <th>mtmltv0323</th>\n",
       "      <th>mtmltv0623</th>\n",
       "      <th>mtmltv0923</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50537</th>\n",
       "      <td>531289.000</td>\n",
       "      <td>34.000</td>\n",
       "      <td>2117.790</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.000</td>\n",
       "      <td>61.000</td>\n",
       "      <td>59.000</td>\n",
       "      <td>59.000</td>\n",
       "      <td>59.000</td>\n",
       "      <td>59.000</td>\n",
       "      <td>59.000</td>\n",
       "      <td>60.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50538</th>\n",
       "      <td>546643.000</td>\n",
       "      <td>34.000</td>\n",
       "      <td>1738.920</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>79.000</td>\n",
       "      <td>77.000</td>\n",
       "      <td>74.000</td>\n",
       "      <td>72.000</td>\n",
       "      <td>72.000</td>\n",
       "      <td>71.000</td>\n",
       "      <td>71.000</td>\n",
       "      <td>71.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50539</th>\n",
       "      <td>512993.000</td>\n",
       "      <td>34.000</td>\n",
       "      <td>2353.260</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>95.000</td>\n",
       "      <td>91.000</td>\n",
       "      <td>88.000</td>\n",
       "      <td>85.000</td>\n",
       "      <td>84.000</td>\n",
       "      <td>84.000</td>\n",
       "      <td>83.000</td>\n",
       "      <td>82.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50540</th>\n",
       "      <td>518631.000</td>\n",
       "      <td>34.000</td>\n",
       "      <td>5283.750</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.000</td>\n",
       "      <td>53.000</td>\n",
       "      <td>50.000</td>\n",
       "      <td>49.000</td>\n",
       "      <td>49.000</td>\n",
       "      <td>49.000</td>\n",
       "      <td>48.000</td>\n",
       "      <td>48.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50541</th>\n",
       "      <td>544740.000</td>\n",
       "      <td>34.000</td>\n",
       "      <td>1738.920</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.000</td>\n",
       "      <td>74.000</td>\n",
       "      <td>69.000</td>\n",
       "      <td>66.000</td>\n",
       "      <td>65.000</td>\n",
       "      <td>64.000</td>\n",
       "      <td>63.000</td>\n",
       "      <td>63.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 543 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          nsmoid  survey_wave  analysis_weight  x05a  x05b  x05c  x05d  x05e  \\\n",
       "50537 531289.000       34.000         2117.790 1.000 1.000 1.000 1.000 1.000   \n",
       "50538 546643.000       34.000         1738.920 3.000 3.000 2.000 2.000 2.000   \n",
       "50539 512993.000       34.000         2353.260 1.000 2.000 2.000 2.000 2.000   \n",
       "50540 518631.000       34.000         5283.750 3.000 3.000 3.000 3.000 3.000   \n",
       "50541 544740.000       34.000         1738.920 1.000 1.000 1.000 1.000 1.000   \n",
       "\n",
       "       x05f  x05g  ...  mtmltv0621  mtmltv0921  mtmltv1221  mtmltv0322  \\\n",
       "50537 1.000 1.000  ...         NaN         NaN      64.000      61.000   \n",
       "50538 1.000 3.000  ...         NaN         NaN      79.000      77.000   \n",
       "50539 1.000 2.000  ...         NaN         NaN      95.000      91.000   \n",
       "50540 3.000 3.000  ...         NaN         NaN      56.000      53.000   \n",
       "50541 1.000 2.000  ...         NaN         NaN      80.000      74.000   \n",
       "\n",
       "       mtmltv0622  mtmltv0922  mtmltv1222  mtmltv0323  mtmltv0623  mtmltv0923  \n",
       "50537      59.000      59.000      59.000      59.000      59.000      60.000  \n",
       "50538      74.000      72.000      72.000      71.000      71.000      71.000  \n",
       "50539      88.000      85.000      84.000      84.000      83.000      82.000  \n",
       "50540      50.000      49.000      49.000      49.000      48.000      48.000  \n",
       "50541      69.000      66.000      65.000      64.000      63.000      63.000  \n",
       "\n",
       "[5 rows x 543 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50a3cc95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x05a</th>\n",
       "      <th>x05b</th>\n",
       "      <th>x05c</th>\n",
       "      <th>x05d</th>\n",
       "      <th>x05e</th>\n",
       "      <th>x05f</th>\n",
       "      <th>x05g</th>\n",
       "      <th>x06</th>\n",
       "      <th>x07</th>\n",
       "      <th>x08a</th>\n",
       "      <th>...</th>\n",
       "      <th>forb0621</th>\n",
       "      <th>forb0921</th>\n",
       "      <th>forb1221</th>\n",
       "      <th>forb0322</th>\n",
       "      <th>forb0622</th>\n",
       "      <th>forb0922</th>\n",
       "      <th>forb1222</th>\n",
       "      <th>forb0323</th>\n",
       "      <th>forb0623</th>\n",
       "      <th>forb0923</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50537</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50538</th>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50539</th>\n",
       "      <td>1.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50540</th>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50541</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 395 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       x05a  x05b  x05c  x05d  x05e  x05f  x05g   x06   x07  x08a  ...  \\\n",
       "50537 1.000 1.000 1.000 1.000 1.000 1.000 1.000 3.000 2.000 1.000  ...   \n",
       "50538 3.000 3.000 2.000 2.000 2.000 1.000 3.000 3.000 3.000 2.000  ...   \n",
       "50539 1.000 2.000 2.000 2.000 2.000 1.000 2.000 3.000 2.000 1.000  ...   \n",
       "50540 3.000 3.000 3.000 3.000 3.000 3.000 3.000 1.000 1.000 1.000  ...   \n",
       "50541 1.000 1.000 1.000 1.000 1.000 1.000 2.000 3.000 2.000 1.000  ...   \n",
       "\n",
       "       forb0621  forb0921  forb1221  forb0322  forb0622  forb0922  forb1222  \\\n",
       "50537       NaN       NaN       NaN       NaN     2.000     2.000     2.000   \n",
       "50538       NaN       NaN       NaN     2.000     2.000     2.000     2.000   \n",
       "50539       NaN       NaN       NaN       NaN     2.000     2.000     2.000   \n",
       "50540       NaN       NaN       NaN       NaN     2.000     2.000     2.000   \n",
       "50541       NaN       NaN       NaN     2.000     2.000     2.000     2.000   \n",
       "\n",
       "       forb0323  forb0623  forb0923  \n",
       "50537     2.000     2.000     2.000  \n",
       "50538     2.000     2.000     2.000  \n",
       "50539     2.000     2.000     2.000  \n",
       "50540     2.000     2.000     2.000  \n",
       "50541     2.000     2.000     2.000  \n",
       "\n",
       "[5 rows x 395 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out a few obs for just the categorical variable\n",
    "\n",
    "raw_df[categorical_variables].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0ed96ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nsmoid</th>\n",
       "      <th>survey_wave</th>\n",
       "      <th>analysis_weight</th>\n",
       "      <th>x74r</th>\n",
       "      <th>rate_spread</th>\n",
       "      <th>pmms</th>\n",
       "      <th>term</th>\n",
       "      <th>ltv</th>\n",
       "      <th>cltv</th>\n",
       "      <th>dti</th>\n",
       "      <th>...</th>\n",
       "      <th>mtmltv0621</th>\n",
       "      <th>mtmltv0921</th>\n",
       "      <th>mtmltv1221</th>\n",
       "      <th>mtmltv0322</th>\n",
       "      <th>mtmltv0622</th>\n",
       "      <th>mtmltv0922</th>\n",
       "      <th>mtmltv1222</th>\n",
       "      <th>mtmltv0323</th>\n",
       "      <th>mtmltv0623</th>\n",
       "      <th>mtmltv0923</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50537</th>\n",
       "      <td>531289.000</td>\n",
       "      <td>34.000</td>\n",
       "      <td>2117.790</td>\n",
       "      <td>57.000</td>\n",
       "      <td>0.640</td>\n",
       "      <td>3.110</td>\n",
       "      <td>40.000</td>\n",
       "      <td>64.000</td>\n",
       "      <td>64.000</td>\n",
       "      <td>42.000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.000</td>\n",
       "      <td>61.000</td>\n",
       "      <td>59.000</td>\n",
       "      <td>59.000</td>\n",
       "      <td>59.000</td>\n",
       "      <td>59.000</td>\n",
       "      <td>59.000</td>\n",
       "      <td>60.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50538</th>\n",
       "      <td>546643.000</td>\n",
       "      <td>34.000</td>\n",
       "      <td>1738.920</td>\n",
       "      <td>37.000</td>\n",
       "      <td>0.030</td>\n",
       "      <td>3.100</td>\n",
       "      <td>30.000</td>\n",
       "      <td>79.000</td>\n",
       "      <td>79.000</td>\n",
       "      <td>33.000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>79.000</td>\n",
       "      <td>77.000</td>\n",
       "      <td>74.000</td>\n",
       "      <td>72.000</td>\n",
       "      <td>72.000</td>\n",
       "      <td>71.000</td>\n",
       "      <td>71.000</td>\n",
       "      <td>71.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50539</th>\n",
       "      <td>512993.000</td>\n",
       "      <td>34.000</td>\n",
       "      <td>2353.260</td>\n",
       "      <td>26.000</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>3.100</td>\n",
       "      <td>30.000</td>\n",
       "      <td>95.000</td>\n",
       "      <td>95.000</td>\n",
       "      <td>35.000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>95.000</td>\n",
       "      <td>91.000</td>\n",
       "      <td>88.000</td>\n",
       "      <td>85.000</td>\n",
       "      <td>84.000</td>\n",
       "      <td>84.000</td>\n",
       "      <td>83.000</td>\n",
       "      <td>82.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50540</th>\n",
       "      <td>518631.000</td>\n",
       "      <td>34.000</td>\n",
       "      <td>5283.750</td>\n",
       "      <td>36.000</td>\n",
       "      <td>-0.230</td>\n",
       "      <td>3.100</td>\n",
       "      <td>20.000</td>\n",
       "      <td>56.000</td>\n",
       "      <td>56.000</td>\n",
       "      <td>46.000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.000</td>\n",
       "      <td>53.000</td>\n",
       "      <td>50.000</td>\n",
       "      <td>49.000</td>\n",
       "      <td>49.000</td>\n",
       "      <td>49.000</td>\n",
       "      <td>48.000</td>\n",
       "      <td>48.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50541</th>\n",
       "      <td>544740.000</td>\n",
       "      <td>34.000</td>\n",
       "      <td>1738.920</td>\n",
       "      <td>42.000</td>\n",
       "      <td>0.080</td>\n",
       "      <td>3.050</td>\n",
       "      <td>30.000</td>\n",
       "      <td>80.000</td>\n",
       "      <td>80.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.000</td>\n",
       "      <td>74.000</td>\n",
       "      <td>69.000</td>\n",
       "      <td>66.000</td>\n",
       "      <td>65.000</td>\n",
       "      <td>64.000</td>\n",
       "      <td>63.000</td>\n",
       "      <td>63.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 148 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          nsmoid  survey_wave  analysis_weight   x74r  rate_spread  pmms  \\\n",
       "50537 531289.000       34.000         2117.790 57.000        0.640 3.110   \n",
       "50538 546643.000       34.000         1738.920 37.000        0.030 3.100   \n",
       "50539 512993.000       34.000         2353.260 26.000       -0.230 3.100   \n",
       "50540 518631.000       34.000         5283.750 36.000       -0.230 3.100   \n",
       "50541 544740.000       34.000         1738.920 42.000        0.080 3.050   \n",
       "\n",
       "        term    ltv   cltv    dti  ...  mtmltv0621  mtmltv0921  mtmltv1221  \\\n",
       "50537 40.000 64.000 64.000 42.000  ...         NaN         NaN      64.000   \n",
       "50538 30.000 79.000 79.000 33.000  ...         NaN         NaN      79.000   \n",
       "50539 30.000 95.000 95.000 35.000  ...         NaN         NaN      95.000   \n",
       "50540 20.000 56.000 56.000 46.000  ...         NaN         NaN      56.000   \n",
       "50541 30.000 80.000 80.000 20.000  ...         NaN         NaN      80.000   \n",
       "\n",
       "       mtmltv0322  mtmltv0622  mtmltv0922  mtmltv1222  mtmltv0323  mtmltv0623  \\\n",
       "50537      61.000      59.000      59.000      59.000      59.000      59.000   \n",
       "50538      77.000      74.000      72.000      72.000      71.000      71.000   \n",
       "50539      91.000      88.000      85.000      84.000      84.000      83.000   \n",
       "50540      53.000      50.000      49.000      49.000      49.000      48.000   \n",
       "50541      74.000      69.000      66.000      65.000      64.000      63.000   \n",
       "\n",
       "       mtmltv0923  \n",
       "50537      60.000  \n",
       "50538      71.000  \n",
       "50539      82.000  \n",
       "50540      48.000  \n",
       "50541      63.000  \n",
       "\n",
       "[5 rows x 148 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out a few obs for just the numeric variable\n",
    "\n",
    "raw_df[numeric_variables].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b812a4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View survey answers for any given observation in a human readable format using the YAML metadata\n",
    "# Deactivated code below because it will print 500+ lines because we have 500+ variables\n",
    "\n",
    "if False:\n",
    "    one_obs = raw_df.iloc[50541]\n",
    "    # Loop through all columns for one obs\n",
    "    for col, value in one_obs.items():\n",
    "        # if it's a categorical variable, then look up the category\n",
    "        if not(pd.isna(value)) and col in categorical_variables:\n",
    "            print(variable_labels_dict[col], \":\", categorical_variables_categories_dict[variable_formats_dict[col]][value])\n",
    "        # else it's a numeric variable or null\n",
    "        else:\n",
    "            print(variable_labels_dict[col], \":\", value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a1085e",
   "metadata": {},
   "source": [
    "# Join with External Data: 30-year Treasury Yields (i.e., the Risk-Free Rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb9d2448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the raw CSV data for Market Yields on U.S. Treasury Securities at 30-Year Constant Maturity\n",
    "# https://fred.stlouisfed.org/series/DGS30\n",
    "\n",
    "treasury_yields_df = pd.read_csv('DGS30.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5771010e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new columns based upon the observation date, which will be used in join to the main data\n",
    "\n",
    "treasury_yields_df['observation_date'] = pd.to_datetime(treasury_yields_df['observation_date'])\n",
    "treasury_yields_df['observation_year'] = treasury_yields_df['observation_date'].dt.year\n",
    "treasury_yields_df['observation_month'] = treasury_yields_df['observation_date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b651d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>observation_date</th>\n",
       "      <th>DGS30</th>\n",
       "      <th>observation_year</th>\n",
       "      <th>observation_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2343</th>\n",
       "      <td>2021-12-27</td>\n",
       "      <td>1.880</td>\n",
       "      <td>2021</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2344</th>\n",
       "      <td>2021-12-28</td>\n",
       "      <td>1.900</td>\n",
       "      <td>2021</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345</th>\n",
       "      <td>2021-12-29</td>\n",
       "      <td>1.960</td>\n",
       "      <td>2021</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2346</th>\n",
       "      <td>2021-12-30</td>\n",
       "      <td>1.930</td>\n",
       "      <td>2021</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2347</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>1.900</td>\n",
       "      <td>2021</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     observation_date  DGS30  observation_year  observation_month\n",
       "2343       2021-12-27  1.880              2021                 12\n",
       "2344       2021-12-28  1.900              2021                 12\n",
       "2345       2021-12-29  1.960              2021                 12\n",
       "2346       2021-12-30  1.930              2021                 12\n",
       "2347       2021-12-31  1.900              2021                 12"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out the daily obsevations of DGS30, which represents the 30-year Treasury Yield (i.e., the Risk-Free Rate)\n",
    "\n",
    "treasury_yields_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3a4e2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average Treasury Yields over each Year and Month\n",
    "\n",
    "average_treasury_yields_df = treasury_yields_df.groupby(['observation_year', 'observation_month'])['DGS30'].mean()\n",
    "average_treasury_yields_df = average_treasury_yields_df.to_frame().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4253e371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>observation_year</th>\n",
       "      <th>observation_month</th>\n",
       "      <th>DGS30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>3.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013</td>\n",
       "      <td>2</td>\n",
       "      <td>3.165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>3.163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013</td>\n",
       "      <td>4</td>\n",
       "      <td>2.933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013</td>\n",
       "      <td>5</td>\n",
       "      <td>3.113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   observation_year  observation_month  DGS30\n",
       "0              2013                  1  3.080\n",
       "1              2013                  2  3.165\n",
       "2              2013                  3  3.163\n",
       "3              2013                  4  2.933\n",
       "4              2013                  5  3.113"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out the average Treasury Yields over each Year and Month\n",
    "\n",
    "average_treasury_yields_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61f7b467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left join the Treasury Yields to the raw Mortgage Origination data using the composite keys of year and month\n",
    "\n",
    "merged_df = pd.merge(left=raw_df, \n",
    "                     right=average_treasury_yields_df,\n",
    "                     left_on=['open_year', 'open_month'],\n",
    "                     right_on=['observation_year', 'observation_month'],\n",
    "                     how='left')\n",
    "\n",
    "# Drop the redundant columns of composite keys used in join\n",
    "\n",
    "merged_df = merged_df.drop(['observation_year', 'observation_month'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5bbce0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nsmoid</th>\n",
       "      <th>survey_wave</th>\n",
       "      <th>analysis_weight</th>\n",
       "      <th>x05a</th>\n",
       "      <th>x05b</th>\n",
       "      <th>x05c</th>\n",
       "      <th>x05d</th>\n",
       "      <th>x05e</th>\n",
       "      <th>x05f</th>\n",
       "      <th>x05g</th>\n",
       "      <th>...</th>\n",
       "      <th>mtmltv0921</th>\n",
       "      <th>mtmltv1221</th>\n",
       "      <th>mtmltv0322</th>\n",
       "      <th>mtmltv0622</th>\n",
       "      <th>mtmltv0922</th>\n",
       "      <th>mtmltv1222</th>\n",
       "      <th>mtmltv0323</th>\n",
       "      <th>mtmltv0623</th>\n",
       "      <th>mtmltv0923</th>\n",
       "      <th>DGS30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>509550.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1207.580</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>503017.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1207.580</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>514904.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1027.740</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>525176.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1711.910</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>540654.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>918.850</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 544 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      nsmoid  survey_wave  analysis_weight  x05a  x05b  x05c  x05d  x05e  \\\n",
       "0 509550.000        1.000         1207.580 3.000 3.000 3.000 3.000 3.000   \n",
       "1 503017.000        1.000         1207.580 1.000 1.000 1.000 1.000 1.000   \n",
       "2 514904.000        1.000         1027.740 1.000 2.000 1.000 1.000 1.000   \n",
       "3 525176.000        1.000         1711.910 1.000 1.000 1.000 1.000 1.000   \n",
       "4 540654.000        1.000          918.850 1.000 1.000 2.000 1.000 2.000   \n",
       "\n",
       "   x05f  x05g  ...  mtmltv0921  mtmltv1221  mtmltv0322  mtmltv0622  \\\n",
       "0 1.000 3.000  ...         NaN         NaN         NaN         NaN   \n",
       "1 1.000 1.000  ...         NaN         NaN         NaN         NaN   \n",
       "2 1.000 1.000  ...         NaN         NaN         NaN         NaN   \n",
       "3 1.000 1.000  ...         NaN         NaN         NaN         NaN   \n",
       "4 1.000 1.000  ...         NaN         NaN         NaN         NaN   \n",
       "\n",
       "   mtmltv0922  mtmltv1222  mtmltv0323  mtmltv0623  mtmltv0923  DGS30  \n",
       "0         NaN         NaN         NaN         NaN         NaN  3.080  \n",
       "1         NaN         NaN         NaN         NaN         NaN  3.165  \n",
       "2         NaN         NaN         NaN         NaN         NaN  3.080  \n",
       "3         NaN         NaN         NaN         NaN         NaN  3.080  \n",
       "4         NaN         NaN         NaN         NaN         NaN  3.080  \n",
       "\n",
       "[5 rows x 544 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out the data after the left join\n",
    "\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9cf0273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivation of Beta, which is a new variable\n",
    "\n",
    "notes = \"\"\"\n",
    "Re = Rf + β(Rm − Rf)\n",
    "Re - Rf = β(Rm − Rf)\n",
    "(Re - Rf) = β(Rm − Rf)\n",
    "(Re - Rf) / (Rm − Rf) = β\n",
    "β = (Re - Rf) / (Rm − Rf)\n",
    "\n",
    "'rate_spread' = Re - 'PMMS'\n",
    "'rate_spread' + 'PMMS' = Re \n",
    "Re = 'rate_spread' + 'PMMS'\n",
    "\n",
    "β = ('rate_spread' + 'PMMS' - Rf) / (Rm − Rf)\n",
    "β = ('rate_spread' + 'PMMS' - 'treasury_yield') / ('PMMS' − 'treasury_yield')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90db6f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation: Beta = ('rate_spread' + 'pmms' - 'treasury_yield') / ('pmms' − 'treasury_yield')\n",
    "\n",
    "merged_df['Beta'] = (merged_df['rate_spread'] + merged_df['pmms'] - merged_df['DGS30']) / (merged_df['pmms'] - merged_df['DGS30'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "222673a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count        50542.000\n",
       "mean       -135974.001\n",
       "std        7048940.930\n",
       "min     -656705858.722\n",
       "1%              -8.000\n",
       "5%              -0.271\n",
       "10%              0.421\n",
       "25%              0.855\n",
       "50%              1.121\n",
       "75%              1.438\n",
       "90%              2.001\n",
       "95%              2.797\n",
       "99%              7.612\n",
       "max      182151991.288\n",
       "Name: Beta, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Describe the distribution of Beta\n",
    "\n",
    "merged_df['Beta'].describe(percentiles=[0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed6a2355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Winsorize the Beta values to reduce the influence of outliers\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mstats.winsorize.html\n",
    "\n",
    "merged_df['Beta_winsorized'] = sp.stats.mstats.winsorize(merged_df['Beta'], limits=[0.05,0.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd9dec0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the new numeric variables to the list of all numeric variables\n",
    "\n",
    "numeric_variables.append('DGS30')\n",
    "numeric_variables.append('Beta')\n",
    "numeric_variables.append('Beta_winsorized')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2274b2",
   "metadata": {},
   "source": [
    "# Process the Data using Dummy Coding, Partitioning, Imputing, and then Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "275230b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables for each category for each categorical variable\n",
    "\n",
    "processed_df = pd.get_dummies(merged_df, columns=categorical_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54994005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the \".0\" in many of the dummy variable due to the columns in the raw data being floats\n",
    "\n",
    "new_columns_list = []\n",
    "for col in processed_df.columns:\n",
    "    new_col = col.replace(\".0\", \"\")\n",
    "    new_columns_list.append(new_col)\n",
    "    \n",
    "processed_df.columns = new_columns_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af84a8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the names of the new categorical variables (i.e., the dummy variables)\n",
    "\n",
    "new_categorical_variables = []\n",
    "for col in processed_df.columns:\n",
    "    if col not in numeric_variables:\n",
    "        new_categorical_variables.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84fd726b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x05a_1</th>\n",
       "      <th>x05a_2</th>\n",
       "      <th>x05a_3</th>\n",
       "      <th>x05b_1</th>\n",
       "      <th>x05b_2</th>\n",
       "      <th>x05b_3</th>\n",
       "      <th>x05c_1</th>\n",
       "      <th>x05c_2</th>\n",
       "      <th>x05c_3</th>\n",
       "      <th>x05d_1</th>\n",
       "      <th>...</th>\n",
       "      <th>forb0922_1</th>\n",
       "      <th>forb0922_2</th>\n",
       "      <th>forb1222_1</th>\n",
       "      <th>forb1222_2</th>\n",
       "      <th>forb0323_1</th>\n",
       "      <th>forb0323_2</th>\n",
       "      <th>forb0623_1</th>\n",
       "      <th>forb0623_2</th>\n",
       "      <th>forb0923_1</th>\n",
       "      <th>forb0923_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50537</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50538</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50539</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50540</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50541</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1545 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       x05a_1  x05a_2  x05a_3  x05b_1  x05b_2  x05b_3  x05c_1  x05c_2  x05c_3  \\\n",
       "50537    True   False   False    True   False   False    True   False   False   \n",
       "50538   False   False    True   False   False    True   False    True   False   \n",
       "50539    True   False   False   False    True   False   False    True   False   \n",
       "50540   False   False    True   False   False    True   False   False    True   \n",
       "50541    True   False   False    True   False   False    True   False   False   \n",
       "\n",
       "       x05d_1  ...  forb0922_1  forb0922_2  forb1222_1  forb1222_2  \\\n",
       "50537    True  ...       False        True       False        True   \n",
       "50538   False  ...       False        True       False        True   \n",
       "50539   False  ...       False        True       False        True   \n",
       "50540   False  ...       False        True       False        True   \n",
       "50541    True  ...       False        True       False        True   \n",
       "\n",
       "       forb0323_1  forb0323_2  forb0623_1  forb0623_2  forb0923_1  forb0923_2  \n",
       "50537       False        True       False        True       False        True  \n",
       "50538       False        True       False        True       False        True  \n",
       "50539       False        True       False        True       False        True  \n",
       "50540       False        True       False        True       False        True  \n",
       "50541       False        True       False        True       False        True  \n",
       "\n",
       "[5 rows x 1545 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out a few obs for just the new categorical variable\n",
    "\n",
    "processed_df[new_categorical_variables].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49929c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the target variable\n",
    "\n",
    "target_variable = 'Beta_winsorized'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27b52aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List variables to be excluded\n",
    "\n",
    "exclude_variables = ['nsmoid',          # NSMO Identification Number\n",
    "                     'survey_wave',     # NSMO Survey Wave (Quarterly)\n",
    "                     'analysis_weight', # NSMO Analysis Weight (Sampling Weight x Non-response Adjustment)\n",
    "                     'rate_spread',     # Mortgage Interest Rate Spread at Origination (Percent)\n",
    "                     'pmms',            # Freddie Mac's Primary Mortgage Market Survey (PMMS) Rate at Origination (Percent)\n",
    "                     'DGS30',           # Market Yield on U.S. Treasury Securities at 30-Year Constant Maturity, Quoted on an Investment Basis\n",
    "                     'Beta'             # Original Beta before it was winsorized\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae483869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Beta_winsorized', 'nsmoid', 'survey_wave', 'analysis_weight', 'rate_spread', 'pmms', 'DGS30', 'Beta']\n"
     ]
    }
   ],
   "source": [
    "# Create a compete list of all excluded variables and exclude them from the list of numeric variables\n",
    "\n",
    "list_of_excluded_variables = list([target_variable]) + exclude_variables\n",
    "\n",
    "for excluded_variable in list_of_excluded_variables:\n",
    "    numeric_variables.remove(excluded_variable)\n",
    "\n",
    "print(list_of_excluded_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a307e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum of missing values in target:  0 \n",
      "total observations in data:  50542 \n",
      "percentage of target with missing values:  0.0 %\n"
     ]
    }
   ],
   "source": [
    "# Calculate the percentage of missing values in the target variable\n",
    "\n",
    "sum_of_missing_values_in_target = processed_df[target_variable].isna().sum()\n",
    "print(\"sum of missing values in target: \", sum_of_missing_values_in_target, \n",
    "      \"\\ntotal observations in data: \", processed_df.shape[0], \n",
    "      \"\\npercentage of target with missing values: \", round(sum_of_missing_values_in_target / processed_df.shape[0] * 100,1), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f0c3fc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop observations in the data where the target variable has missing values\n",
    "\n",
    "processed_df = processed_df.dropna(subset=target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9fbdda4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segregate the predictor variables from the target variable\n",
    "\n",
    "X = processed_df.drop(columns=list_of_excluded_variables)\n",
    "y = processed_df[target_variable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c84dd57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing partitions\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.80, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "16ba2a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values for the numeric variables using the mean values\n",
    "\n",
    "# Instantiate an imputer object\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Prevent \"data leakage\" by fitting the imputer on just the training data, and then apply it to the holdout testing data\n",
    "imputer.fit(X_train[numeric_variables])\n",
    "\n",
    "# Transform both the training and testing data using the imputer fitted on just the training data\n",
    "X_train[numeric_variables] = imputer.transform(X_train[numeric_variables])\n",
    "X_test[numeric_variables] = imputer.transform(X_test[numeric_variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "915ccd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale values for the numeric variables\n",
    "\n",
    "# Instantiate a scaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Prevent \"data leakage\" by fitting the scaler on just the imputed training data, and then apply it to the holdout testing data\n",
    "scaler.fit(X_train[numeric_variables])\n",
    "\n",
    "# Scale both the training data and testing data using the scaler fitted on just the training data\n",
    "X_train[numeric_variables] = scaler.transform(X_train[numeric_variables])\n",
    "X_test[numeric_variables] = scaler.transform(X_test[numeric_variables])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffa91b4",
   "metadata": {},
   "source": [
    "# Supervised Learning Models: Gradient Boosting, Linear, LASSO, and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e615a080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a Gradient Boosting Regression model on the training partition, and then evaluate it on the testing partition\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html\n",
    "\n",
    "params = {'max_depth': 4, 'learning_rate': 0.01, 'random_state':0}\n",
    "grad_boost_reg = HistGradientBoostingRegressor(**params)\n",
    "grad_boost_reg.fit(X_train, y_train)\n",
    "\n",
    "mae = mean_absolute_error(y_test, grad_boost_reg.predict(X_test))\n",
    "mse = mean_squared_error(y_test, grad_boost_reg.predict(X_test))\n",
    "rmse = np.sqrt(mse)\n",
    "grad_boost_reg_dict = {\"model\": \"Gradient Boosting Regression\", \n",
    "                       \"Mean Absolute Error\": mae, \n",
    "                       \"Root Mean Squared Error\": rmse}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "52264c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a Linear Model on the training partition, and evaluate it on the testing partition\n",
    "# There is a lot of Multicollinearity in this model because we are putting all predictor variables into the model\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "\n",
    "linear_reg = linear_model.LinearRegression()\n",
    "linear_reg.fit(X_train,y_train)\n",
    "\n",
    "mae = mean_absolute_error(y_test, linear_reg.predict(X_test))\n",
    "mse = mean_squared_error(y_test, linear_reg.predict(X_test))\n",
    "rmse = np.sqrt(mse)\n",
    "linear_reg_dict = {\"model\": \"Linear Regression\", \n",
    "                   \"Mean Absolute Error\": mae, \n",
    "                   \"Root Mean Squared Error\": rmse}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eb666ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a LASSO regression on the training partition, and then evaluate it on the testing partition\n",
    "# This will shrink most of the variable coefficients to zero for automated variable selection\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso\n",
    "# Default hyperparameters: alpha=1.0, max_iter=1000, tol=0.0001\n",
    "\n",
    "linear_reg_with_lasso = linear_model.Lasso(alpha=1.0, max_iter=1000, tol=0.0001)\n",
    "linear_reg_with_lasso.fit(X_train,y_train)\n",
    "\n",
    "mae = mean_absolute_error(y_test, linear_reg_with_lasso.predict(X_test))\n",
    "mse = mean_squared_error(y_test, linear_reg_with_lasso.predict(X_test))\n",
    "rmse = np.sqrt(mse)\n",
    "lasso_reg_dict = {\"model\": \"LASSO Regression\", \n",
    "                   \"Mean Absolute Error\": mae, \n",
    "                   \"Root Mean Squared Error\": rmse}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a64a0b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the LASSO regression to identify the most useful variable to predict the target (i.e., the variables whose coefficient didn't shrink to zero)\n",
    "\n",
    "variable_names = list(X.columns) \n",
    "linear_reg_with_lasso_coef = linear_reg_with_lasso.coef_\n",
    "\n",
    "for variable, coef in zip(variable_names, linear_reg_with_lasso_coef):\n",
    "    # if the coefficient didn't shrink to zero\n",
    "    if (coef != 0):\n",
    "        # if it's a categorical variable\n",
    "        if variable in new_categorical_variables:\n",
    "            variable_substrings = variable.split(\"_\")  # split up the new categorical variable name by underscores\n",
    "            categorical_variable = '_'.join(variable_substrings[:-1])  # retrieve the original categorical variable name (e.g., 'x05a', 'perf_status_0923', etc.)\n",
    "            category = variable_substrings[-1:][0]  # retrieve the category chosen for the categorical variable (e.g., '1', '10', '2013', 'A', etc.)\n",
    "            if any(character.isdigit() for character in category):\n",
    "                category = int(category)\n",
    "            else:\n",
    "                category = str(category)\n",
    "            print(variable, \":\", variable_labels_dict[categorical_variable], \":\", categorical_variables_categories_dict[variable_formats_dict[categorical_variable]][category], \":\", coef)\n",
    "        # else it's a numeric variable\n",
    "        else:\n",
    "            print(variable, \":\", variable_labels_dict[variable], \":\", coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ecb77c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a Random Forest regression model on the training partition, and then evaluate it on the testing partition\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "\n",
    "params = {'n_estimators': 100, 'max_depth': 4, 'random_state':0}\n",
    "random_forest_reg = RandomForestRegressor(**params)\n",
    "random_forest_reg.fit(X_train, y_train)\n",
    "\n",
    "mae = mean_absolute_error(y_test, random_forest_reg.predict(X_test))\n",
    "mse = mean_squared_error(y_test, random_forest_reg.predict(X_test))\n",
    "rmse = np.sqrt(mse)\n",
    "random_forest_reg_dict = {\"model\": \"Random Forest Regression\", \n",
    "                          \"Mean Absolute Error\": mae, \n",
    "                          \"Root Mean Squared Error\": rmse}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dc708990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loan_type_3 : Mortgage Type : VA guaranteed : 0.21258354368654667\n",
      "term : Mortgage Term (in Years) at Origination : 0.1387406552360419\n",
      "score_orig_r : VantageScore 3.0 at Origination | Respondent : 0.11430423353821635\n",
      "x17g_1_1 : How important were each of the following in determining the mortgage you took out? | A term of less than 30 years (Waves 1-6) : Very : 0.09295086650890233\n",
      "loan_type_1 : Mortgage Type : Conventional : 0.056328513006065985\n",
      "loan_amount_cat_2 : Mortgage Loan Amount at Origination (Categorical) : $50,000 to  $99,999 : 0.055071642263274725\n",
      "x66_5 : Which one of the following best describes how you use this property? : Rental or investment property : 0.05481968470317694\n",
      "perf_status_0614_P : Mortgage Performance Status in June 2014 : Performance history not yet started (before loan opened) : 0.049578835455183295\n",
      "perf_status_0914_P : Mortgage Performance Status in September 2014 : Performance history not yet started (before loan opened) : 0.03355661658847897\n",
      "x44_1 : Does this mortgage have... | An adjustable rate (one that can change over the life of the loan)? : Yes : 0.03353701269623925\n",
      "mtmltv0914 : Mark-to-Market Loan-to-Value Ratio of the Mortgage, September 2014 : 0.02374832639116061\n",
      "x37e_1_1 : How important, if at all, were the following reasons in your decision to refinance, modify, or obtain a new mortgage? | Repay the loan more quickly (Waves 1-6) : Very : 0.01717499143704608\n",
      "perf_status_1214_Q : Mortgage Performance Status in December 2014 : Loan opened during this quarter : 0.01589391000475116\n",
      "loan_amount_cat_1 : Mortgage Loan Amount at Origination (Categorical) : Less than $50,000 : 0.00889178666651368\n",
      "mtmltv0613 : Mark-to-Market Loan-to-Value Ratio of the Mortgage, June 2013 : 0.005809580443395444\n",
      "perf_status_0913_C : Mortgage Performance Status in September 2013 : Current : 0.0046833473828469425\n",
      "score_0319_r : VantageScore 3.0 in March 2019 | Respondent : 0.004292936714031479\n",
      "x27b_1 : Overall, how satisfied are you that the mortgage you got was the one with the... | Lowest interest rate forwhich you could qualify : Very : 0.004282446944513729\n",
      "perf_status_0613_P : Mortgage Performance Status in June 2013 : Performance history not yet started (before loan opened) : 0.003846434889840061\n",
      "open_month_8 : Mortgage Origination Month : August : 0.003392932519372293\n"
     ]
    }
   ],
   "source": [
    "# Use the Random Forest regression model to identify the top N most important feature based upon their respective feature importance values\n",
    "\n",
    "top_N = 20\n",
    "feature_importances = pd.Series(data=random_forest_reg.feature_importances_, index=X_train.columns)\n",
    "feature_importances.sort_values(ascending=False, inplace=True)\n",
    "top_N_features = feature_importances.head(top_N)\n",
    "\n",
    "# Print the top N feature by importance from Random Forest regression model in descending order\n",
    "for variable in top_N_features.index:\n",
    "    feature_importance = top_N_features[variable]\n",
    "    # if the feature importance is greater than zero\n",
    "    if (feature_importance > 0):\n",
    "        # if it's a categorical variable\n",
    "        if variable in new_categorical_variables:\n",
    "            variable_substrings = variable.split(\"_\")  # split up the new categorical variable name by underscores\n",
    "            categorical_variable = '_'.join(variable_substrings[:-1])  # retrieve the original categorical variable name (e.g., 'x05a', 'perf_status_0923', etc.)\n",
    "            category = variable_substrings[-1:][0]  # retrieve the category chosen for the categorical variable (e.g., '1', '10', '2013', 'A', etc.)\n",
    "            if any(character.isdigit() for character in category):\n",
    "                category = int(category)\n",
    "            else:\n",
    "                category = str(category)\n",
    "            print(variable, \":\", variable_labels_dict[categorical_variable], \":\", categorical_variables_categories_dict[variable_formats_dict[categorical_variable]][category], \":\", feature_importance)\n",
    "        # else it's a numeric variable\n",
    "        else:\n",
    "            print(variable, \":\", variable_labels_dict[variable], \":\", feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b2855b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          model  Mean Absolute Error  Root Mean Squared Error\n",
      "0  Gradient Boosting Regression                0.449                    0.645\n",
      "1             Linear Regression                0.438                    0.631\n",
      "2              LASSO Regression                0.477                    0.673\n",
      "3      Random Forest Regression                0.445                    0.640\n"
     ]
    }
   ],
   "source": [
    "# Compare the performance metrics for the various models on the holdout testing data\n",
    "\n",
    "all_model_performance_metrics = [grad_boost_reg_dict,\n",
    "                                linear_reg_dict,\n",
    "                                lasso_reg_dict,\n",
    "                                random_forest_reg_dict]\n",
    "\n",
    "all_model_performance_metrics_df = pd.DataFrame(all_model_performance_metrics)\n",
    "print(all_model_performance_metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656e4edc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1f4c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd07197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Gradient Boosting hyperparameter tuning...\n",
      "Training set size: (40433, 1688)\n",
      "Total model fits to perform: 135\n",
      "\n",
      "Fitting models...\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "[CV] END .....learning_rate=0.02, max_depth=10, max_iter=500; total time=  33.7s\n",
      "[CV] END .....learning_rate=0.02, max_depth=10, max_iter=500; total time=  37.4s\n",
      "[CV] END .....learning_rate=0.02, max_depth=10, max_iter=500; total time=  39.3s\n",
      "[CV] END .....learning_rate=0.02, max_depth=10, max_iter=500; total time=  39.3s\n",
      "[CV] END .....learning_rate=0.02, max_depth=10, max_iter=500; total time=  40.7s\n",
      "[CV] END ....learning_rate=0.02, max_depth=10, max_iter=1000; total time=  36.6s\n",
      "[CV] END ....learning_rate=0.02, max_depth=10, max_iter=1000; total time=  39.2s\n",
      "[CV] END ....learning_rate=0.02, max_depth=10, max_iter=1000; total time=  40.8s\n",
      "[CV] END ....learning_rate=0.02, max_depth=10, max_iter=1000; total time=  38.7s\n",
      "[CV] END ....learning_rate=0.02, max_depth=10, max_iter=1000; total time=  43.2s\n",
      "[CV] END ....learning_rate=0.02, max_depth=10, max_iter=1500; total time=  33.2s\n",
      "[CV] END ....learning_rate=0.02, max_depth=10, max_iter=1500; total time=  37.5s\n",
      "[CV] END ....learning_rate=0.02, max_depth=10, max_iter=1500; total time=  39.3s\n",
      "[CV] END ....learning_rate=0.02, max_depth=10, max_iter=1500; total time=  43.7s\n",
      "[CV] END ....learning_rate=0.02, max_depth=10, max_iter=1500; total time=  37.7s\n",
      "[CV] END .....learning_rate=0.02, max_depth=15, max_iter=500; total time=  38.8s\n",
      "[CV] END .....learning_rate=0.02, max_depth=15, max_iter=500; total time=  38.5s\n",
      "[CV] END .....learning_rate=0.02, max_depth=15, max_iter=500; total time=  35.1s\n",
      "[CV] END .....learning_rate=0.02, max_depth=15, max_iter=500; total time=  39.4s\n",
      "[CV] END .....learning_rate=0.02, max_depth=15, max_iter=500; total time=  33.7s\n",
      "[CV] END ....learning_rate=0.02, max_depth=15, max_iter=1000; total time=  41.5s\n",
      "[CV] END ....learning_rate=0.02, max_depth=15, max_iter=1000; total time=  38.6s\n",
      "[CV] END ....learning_rate=0.02, max_depth=15, max_iter=1000; total time=  35.3s\n",
      "[CV] END ....learning_rate=0.02, max_depth=15, max_iter=1000; total time=  44.7s\n",
      "[CV] END ....learning_rate=0.02, max_depth=15, max_iter=1000; total time=  33.8s\n",
      "[CV] END ....learning_rate=0.02, max_depth=15, max_iter=1500; total time=  42.3s\n",
      "[CV] END ....learning_rate=0.02, max_depth=15, max_iter=1500; total time=  39.7s\n",
      "[CV] END ....learning_rate=0.02, max_depth=15, max_iter=1500; total time=  37.2s\n",
      "[CV] END ....learning_rate=0.02, max_depth=15, max_iter=1500; total time=  46.9s\n",
      "[CV] END ....learning_rate=0.02, max_depth=15, max_iter=1500; total time=  34.1s\n",
      "[CV] END .....learning_rate=0.02, max_depth=20, max_iter=500; total time=  37.3s\n",
      "[CV] END .....learning_rate=0.02, max_depth=20, max_iter=500; total time=  37.6s\n",
      "[CV] END .....learning_rate=0.02, max_depth=20, max_iter=500; total time=  39.1s\n",
      "[CV] END .....learning_rate=0.02, max_depth=20, max_iter=500; total time=  39.2s\n",
      "[CV] END .....learning_rate=0.02, max_depth=20, max_iter=500; total time=  35.0s\n",
      "[CV] END ....learning_rate=0.02, max_depth=20, max_iter=1000; total time=  41.4s\n",
      "[CV] END ....learning_rate=0.02, max_depth=20, max_iter=1000; total time=  40.0s\n",
      "[CV] END ....learning_rate=0.02, max_depth=20, max_iter=1000; total time=  40.2s\n",
      "[CV] END ....learning_rate=0.02, max_depth=20, max_iter=1000; total time=  42.5s\n",
      "[CV] END ....learning_rate=0.02, max_depth=20, max_iter=1000; total time=  34.3s\n",
      "[CV] END ....learning_rate=0.02, max_depth=20, max_iter=1500; total time=  41.3s\n",
      "[CV] END ....learning_rate=0.02, max_depth=20, max_iter=1500; total time=  40.1s\n",
      "[CV] END ....learning_rate=0.02, max_depth=20, max_iter=1500; total time=  40.2s\n",
      "[CV] END ....learning_rate=0.02, max_depth=20, max_iter=1500; total time=  42.4s\n",
      "[CV] END ....learning_rate=0.02, max_depth=20, max_iter=1500; total time=  34.1s\n",
      "[CV] END .....learning_rate=0.03, max_depth=10, max_iter=500; total time=  30.0s\n",
      "[CV] END .....learning_rate=0.03, max_depth=10, max_iter=500; total time=  21.7s\n",
      "[CV] END .....learning_rate=0.03, max_depth=10, max_iter=500; total time=  28.2s\n",
      "[CV] END .....learning_rate=0.03, max_depth=10, max_iter=500; total time=  27.3s\n",
      "[CV] END .....learning_rate=0.03, max_depth=10, max_iter=500; total time=  28.4s\n",
      "[CV] END ....learning_rate=0.03, max_depth=10, max_iter=1000; total time=  28.3s\n",
      "[CV] END ....learning_rate=0.03, max_depth=10, max_iter=1000; total time=  28.0s\n",
      "[CV] END ....learning_rate=0.03, max_depth=10, max_iter=1000; total time=  21.4s\n",
      "[CV] END ....learning_rate=0.03, max_depth=10, max_iter=1000; total time=  27.0s\n",
      "[CV] END ....learning_rate=0.03, max_depth=10, max_iter=1000; total time=  28.3s\n",
      "[CV] END ....learning_rate=0.03, max_depth=10, max_iter=1500; total time=  28.3s\n",
      "[CV] END ....learning_rate=0.03, max_depth=10, max_iter=1500; total time=  28.6s\n",
      "[CV] END ....learning_rate=0.03, max_depth=10, max_iter=1500; total time=  21.9s\n",
      "[CV] END ....learning_rate=0.03, max_depth=10, max_iter=1500; total time=  27.0s\n",
      "[CV] END ....learning_rate=0.03, max_depth=10, max_iter=1500; total time=  28.5s\n",
      "[CV] END .....learning_rate=0.03, max_depth=15, max_iter=500; total time=  28.9s\n",
      "[CV] END .....learning_rate=0.03, max_depth=15, max_iter=500; total time=  27.4s\n",
      "[CV] END .....learning_rate=0.03, max_depth=15, max_iter=500; total time=  26.9s\n",
      "[CV] END .....learning_rate=0.03, max_depth=15, max_iter=500; total time=  29.3s\n",
      "[CV] END .....learning_rate=0.03, max_depth=15, max_iter=500; total time=  25.0s\n",
      "[CV] END ....learning_rate=0.03, max_depth=15, max_iter=1000; total time=  30.6s\n",
      "[CV] END ....learning_rate=0.03, max_depth=15, max_iter=1000; total time=  30.3s\n",
      "[CV] END ....learning_rate=0.03, max_depth=15, max_iter=1000; total time=  29.9s\n",
      "[CV] END ....learning_rate=0.03, max_depth=15, max_iter=1000; total time=  30.8s\n",
      "[CV] END ....learning_rate=0.03, max_depth=15, max_iter=1000; total time=  25.1s\n",
      "[CV] END ....learning_rate=0.03, max_depth=15, max_iter=1500; total time=  31.5s\n",
      "[CV] END ....learning_rate=0.03, max_depth=15, max_iter=1500; total time=  30.3s\n",
      "[CV] END ....learning_rate=0.03, max_depth=15, max_iter=1500; total time=  26.0s\n",
      "[CV] END ....learning_rate=0.03, max_depth=15, max_iter=1500; total time=  28.1s\n",
      "[CV] END ....learning_rate=0.03, max_depth=15, max_iter=1500; total time=  23.9s\n",
      "[CV] END .....learning_rate=0.03, max_depth=20, max_iter=500; total time=  28.1s\n",
      "[CV] END .....learning_rate=0.03, max_depth=20, max_iter=500; total time=  34.3s\n",
      "[CV] END .....learning_rate=0.03, max_depth=20, max_iter=500; total time=  33.3s\n",
      "[CV] END .....learning_rate=0.03, max_depth=20, max_iter=500; total time=  27.5s\n",
      "[CV] END .....learning_rate=0.03, max_depth=20, max_iter=500; total time=  24.6s\n",
      "[CV] END ....learning_rate=0.03, max_depth=20, max_iter=1000; total time=  25.5s\n",
      "[CV] END ....learning_rate=0.03, max_depth=20, max_iter=1000; total time=  28.6s\n",
      "[CV] END ....learning_rate=0.03, max_depth=20, max_iter=1000; total time=  29.0s\n",
      "[CV] END ....learning_rate=0.03, max_depth=20, max_iter=1000; total time=  27.4s\n",
      "[CV] END ....learning_rate=0.03, max_depth=20, max_iter=1000; total time=  25.8s\n",
      "[CV] END ....learning_rate=0.03, max_depth=20, max_iter=1500; total time=  26.9s\n",
      "[CV] END ....learning_rate=0.03, max_depth=20, max_iter=1500; total time=  31.0s\n",
      "[CV] END ....learning_rate=0.03, max_depth=20, max_iter=1500; total time=  33.4s\n",
      "[CV] END ....learning_rate=0.03, max_depth=20, max_iter=1500; total time=  35.4s\n",
      "[CV] END ....learning_rate=0.03, max_depth=20, max_iter=1500; total time=  31.2s\n",
      "[CV] END .....learning_rate=0.04, max_depth=10, max_iter=500; total time=  19.5s\n",
      "[CV] END .....learning_rate=0.04, max_depth=10, max_iter=500; total time=  21.4s\n",
      "[CV] END .....learning_rate=0.04, max_depth=10, max_iter=500; total time=  17.7s\n",
      "[CV] END .....learning_rate=0.04, max_depth=10, max_iter=500; total time=  26.4s\n",
      "[CV] END .....learning_rate=0.04, max_depth=10, max_iter=500; total time=  19.8s\n",
      "[CV] END ....learning_rate=0.04, max_depth=10, max_iter=1000; total time=  20.6s\n",
      "[CV] END ....learning_rate=0.04, max_depth=10, max_iter=1000; total time=  21.7s\n",
      "[CV] END ....learning_rate=0.04, max_depth=10, max_iter=1000; total time=  17.4s\n",
      "[CV] END ....learning_rate=0.04, max_depth=10, max_iter=1000; total time=  26.0s\n",
      "[CV] END ....learning_rate=0.04, max_depth=10, max_iter=1000; total time=  19.6s\n",
      "[CV] END ....learning_rate=0.04, max_depth=10, max_iter=1500; total time=  20.1s\n",
      "[CV] END ....learning_rate=0.04, max_depth=10, max_iter=1500; total time=  21.2s\n",
      "[CV] END ....learning_rate=0.04, max_depth=10, max_iter=1500; total time=  17.6s\n",
      "[CV] END ....learning_rate=0.04, max_depth=10, max_iter=1500; total time=  26.3s\n",
      "[CV] END ....learning_rate=0.04, max_depth=10, max_iter=1500; total time=  19.6s\n",
      "[CV] END .....learning_rate=0.04, max_depth=15, max_iter=500; total time=  22.9s\n",
      "[CV] END .....learning_rate=0.04, max_depth=15, max_iter=500; total time=  23.4s\n",
      "[CV] END .....learning_rate=0.04, max_depth=15, max_iter=500; total time=  20.2s\n",
      "[CV] END .....learning_rate=0.04, max_depth=15, max_iter=500; total time=  23.3s\n",
      "[CV] END .....learning_rate=0.04, max_depth=15, max_iter=500; total time=  20.4s\n",
      "[CV] END ....learning_rate=0.04, max_depth=15, max_iter=1000; total time=  23.1s\n",
      "[CV] END ....learning_rate=0.04, max_depth=15, max_iter=1000; total time=  22.6s\n",
      "[CV] END ....learning_rate=0.04, max_depth=15, max_iter=1000; total time=  20.1s\n",
      "[CV] END ....learning_rate=0.04, max_depth=15, max_iter=1000; total time=  23.6s\n",
      "[CV] END ....learning_rate=0.04, max_depth=15, max_iter=1000; total time=  21.0s\n",
      "[CV] END ....learning_rate=0.04, max_depth=15, max_iter=1500; total time=  23.2s\n",
      "[CV] END ....learning_rate=0.04, max_depth=15, max_iter=1500; total time=  22.6s\n",
      "[CV] END ....learning_rate=0.04, max_depth=15, max_iter=1500; total time=  20.0s\n",
      "[CV] END ....learning_rate=0.04, max_depth=15, max_iter=1500; total time=  23.1s\n",
      "[CV] END ....learning_rate=0.04, max_depth=15, max_iter=1500; total time=  20.1s\n",
      "[CV] END .....learning_rate=0.04, max_depth=20, max_iter=500; total time=  22.5s\n",
      "[CV] END .....learning_rate=0.04, max_depth=20, max_iter=500; total time=  21.1s\n",
      "[CV] END .....learning_rate=0.04, max_depth=20, max_iter=500; total time=  16.8s\n",
      "[CV] END .....learning_rate=0.04, max_depth=20, max_iter=500; total time=  20.2s\n",
      "[CV] END .....learning_rate=0.04, max_depth=20, max_iter=500; total time=  24.5s\n",
      "[CV] END ....learning_rate=0.04, max_depth=20, max_iter=1000; total time=  22.3s\n",
      "[CV] END ....learning_rate=0.04, max_depth=20, max_iter=1000; total time=  20.6s\n",
      "[CV] END ....learning_rate=0.04, max_depth=20, max_iter=1000; total time=  16.8s\n",
      "[CV] END ....learning_rate=0.04, max_depth=20, max_iter=1000; total time=  25.6s\n",
      "[CV] END ....learning_rate=0.04, max_depth=20, max_iter=1000; total time=  19.8s\n",
      "[CV] END ....learning_rate=0.04, max_depth=20, max_iter=1500; total time=  23.1s\n",
      "[CV] END ....learning_rate=0.04, max_depth=20, max_iter=1500; total time=  21.4s\n",
      "[CV] END ....learning_rate=0.04, max_depth=20, max_iter=1500; total time=  17.6s\n",
      "[CV] END ....learning_rate=0.04, max_depth=20, max_iter=1500; total time=  26.9s\n",
      "[CV] END ....learning_rate=0.04, max_depth=20, max_iter=1500; total time=  19.9s\n",
      "\n",
      "Best parameters found: {'learning_rate': 0.03, 'max_depth': 20, 'max_iter': 500}\n",
      "Best CV score: 0.393\n",
      "\n",
      "Performing cross-validation on best model...\n"
     ]
    }
   ],
   "source": [
    "# Cross-Validation Setup\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "# Set up 5-fold cross-validation\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "print(\"Starting Gradient Boosting hyperparameter tuning...\")\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "\n",
    "# Hyperparameter tuning for Gradient Boosting\n",
    "# Reduced parameter grid to avoid memory issues\n",
    "gb_param_grid = {\n",
    "    'max_depth': [10, 15, 20],\n",
    "    'learning_rate': [.02, .03, .04],\n",
    "    'max_iter': [500, 1000, 1500]\n",
    "}\n",
    "\n",
    "total_fits = len(gb_param_grid['max_depth']) * len(gb_param_grid['learning_rate']) * len(gb_param_grid['max_iter']) * 5  # 5-fold CV\n",
    "print(f\"Total model fits to perform: {total_fits}\")\n",
    "\n",
    "# Use verbose=2 to see progress, n_jobs=2 to avoid memory issues\n",
    "gb_grid = GridSearchCV(\n",
    "    HistGradientBoostingRegressor(random_state=0),\n",
    "    gb_param_grid,\n",
    "    cv=cv,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_jobs=2,  \n",
    "    verbose=2  # Shows progress\n",
    ")\n",
    "\n",
    "print(\"\\nFitting models...\")\n",
    "gb_grid.fit(X_train, y_train)\n",
    "grad_boost_reg = gb_grid.best_estimator_\n",
    "\n",
    "print(f\"\\nBest parameters found: {gb_grid.best_params_}\")\n",
    "print(f\"Best CV score: {-gb_grid.best_score_:.3f}\")\n",
    "\n",
    "# Evaluate with cross-validation on best model\n",
    "print(\"\\nPerforming cross-validation on best model...\")\n",
    "cv_scores = cross_val_score(grad_boost_reg, X_train, y_train, cv=cv, \n",
    "                           scoring='neg_mean_absolute_error', verbose=1)\n",
    "print(f\"CV MAE: {-cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "\n",
    "# Test set evaluation\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "mae = mean_absolute_error(y_test, grad_boost_reg.predict(X_test))\n",
    "rmse = np.sqrt(mean_squared_error(y_test, grad_boost_reg.predict(X_test)))\n",
    "\n",
    "print(f\"Test MAE: {mae:.3f}\")\n",
    "print(f\"Test RMSE: {rmse:.3f}\")\n",
    "\n",
    "grad_boost_cv_dict = {\n",
    "    \"model\": \"Gradient Boosting Regression (Cross-Validation)\", \n",
    "    \"Mean Absolute Error\": mae, \n",
    "    \"Root Mean Squared Error\": rmse,\n",
    "    \"Best Parameters\": gb_grid.best_params_\n",
    "}\n",
    "print(\"\\nGradient Boosting tuning complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4d0c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             model  Mean Absolute Error  \\\n",
      "0  Gradient Boosting Regression (Cross-Validation)                0.397   \n",
      "1                     Gradient Boosting Regression                0.449   \n",
      "2                                Linear Regression                0.438   \n",
      "3                                 LASSO Regression                0.477   \n",
      "4                         Random Forest Regression                0.445   \n",
      "\n",
      "   Root Mean Squared Error                                    Best Parameters  \n",
      "0                    0.584  {'learning_rate': 0.025, 'max_depth': 10, 'max...  \n",
      "1                    0.645                                                NaN  \n",
      "2                    0.631                                                NaN  \n",
      "3                    0.673                                                NaN  \n",
      "4                    0.640                                                NaN  \n"
     ]
    }
   ],
   "source": [
    "# Compare the performance metrics for the various models on the holdout testing data\n",
    "\n",
    "all_model_performance_metrics = [grad_boost_cv_dict,\n",
    "                                grad_boost_reg_dict,    \n",
    "                                linear_reg_dict,\n",
    "                                lasso_reg_dict,\n",
    "                                random_forest_reg_dict,]\n",
    "\n",
    "all_model_performance_metrics_df = pd.DataFrame(all_model_performance_metrics)\n",
    "print(all_model_performance_metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953a1674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'Gradient Boosting Regression (Cross-Validation)', 'Mean Absolute Error': 0.3973034528119412, 'Root Mean Squared Error': np.float64(0.5843658246868284), 'Best Parameters': {'learning_rate': 0.025, 'max_depth': 10, 'max_iter': 500}}\n"
     ]
    }
   ],
   "source": [
    "print(grad_boost_cv_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20210c74",
   "metadata": {},
   "source": [
    "# Unsupervised Learning: K-means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "720214a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate X_train and X_test as X for Unsupervised Learning\n",
    "\n",
    "X = pd.concat([X_train, X_test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "168bfae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute K-means Clustering from K = 1 to 10 using Mini Batches to speed up computations\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html\n",
    "\n",
    "list_of_inertia_for_each_K = []\n",
    "\n",
    "for k in range(1, 11):\n",
    "    mbk = MiniBatchKMeans(init ='k-means++', \n",
    "                          n_clusters = k,\n",
    "                          batch_size = 100, \n",
    "                          n_init = 10,\n",
    "                          max_no_improvement = 10, \n",
    "                          verbose = 0)\n",
    "\n",
    "    mbk.fit(X)\n",
    "    list_of_inertia_for_each_K.append(mbk.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6b1c5c79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot of the Number of Clusters vs. the Inertia of Mini Batch K-means Clustering\n",
    "\n",
    "plt.plot(range(1, 11), list_of_inertia_for_each_K)\n",
    "plt.xticks(range(1, 11))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Inertia (Sum of the Squared Distances)\")\n",
    "plt.title(\"Number of Clusters vs. the Inertia (Sum of the Squared Distances)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9650922f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8640fd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 1 ... 2 1 2]\n"
     ]
    }
   ],
   "source": [
    "# K-means Clustering with K=4 seems optimal because of the \"elbow\" point\n",
    "# This probably adjusts to 3 with the updated info\n",
    "\n",
    "# https://www.geeksforgeeks.org/ml-mini-batch-k-means-clustering-algorithm/\n",
    "\n",
    "mbk = MiniBatchKMeans(init ='k-means++', \n",
    "                      n_clusters = 4,\n",
    "                      batch_size = 100, \n",
    "                      n_init = 10,\n",
    "                      max_no_improvement = 10, \n",
    "                      verbose = 0)\n",
    "\n",
    "mbk.fit(X)\n",
    "mbk_means_cluster_centers = np.sort(mbk.cluster_centers_, axis = 0)\n",
    "mbk_means_labels = pairwise_distances_argmin(X, mbk_means_cluster_centers)\n",
    "\n",
    "# Print out the cluster labels for each observation\n",
    "\n",
    "print(mbk_means_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0f9803",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419c390e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use your existing scaled data\n",
    "print(\"Starting PCA...\")\n",
    "\n",
    "# Option: Specify the percentage of variance to retain (e.g., 95%)\n",
    "pca = PCA(n_components=0.95)\n",
    "\n",
    "# Fit PCA on your concatenated scaled data\n",
    "X_pca = pca.fit_transform(X)  # X is your concatenated train+test data\n",
    "\n",
    "# Rest of the PCA code remains the same...\n",
    "print(f\"Number of components selected: {pca.n_components_}\")\n",
    "print(\"Explained variance ratio per component:\")\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "# Cumulative explained variance\n",
    "cumulative_explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "print(\"\\nCumulative explained variance:\")\n",
    "print(cumulative_explained_variance)\n",
    "\n",
    "# Plotting the explained variance (Scree Plot)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, pca.n_components_ + 1), pca.explained_variance_ratio_, marker='o', linestyle='--', label='Individual explained variance')\n",
    "plt.plot(range(1, pca.n_components_ + 1), cumulative_explained_variance, marker='o', linestyle='-', label='Cumulative explained variance')\n",
    "plt.title('Scree Plot - Explained Variance by Principal Components')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Proportion of Variance Explained')\n",
    "plt.xticks(range(1, pca.n_components_ + 1)) # Ensure x-ticks match number of components\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# The transformed data X_pca can now be used for further analysis or as input to clustering algorithms\n",
    "print(f\"\\nShape of data after PCA: {X_pca.shape}\")\n",
    "# PCA DataFrame\n",
    "X_pca_df = pd.DataFrame(data=X_pca, columns=[f'PC{i+1}' for i in range(X_pca.shape[1])])\n",
    "print(X_pca_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c01571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'X_scaled' is your scaled feature matrix.\n",
    "# Or, you might want to use 'X_pca' from the PCA step above.\n",
    "# For demonstration, let's create a dummy X_scaled:\n",
    "# X_scaled = pd.DataFrame(np.random.rand(100, 10)) # Replace with your actual X_scaled or X_pca\n",
    "\n",
    "# --- Start of GMM ---\n",
    "print(\"Starting GMM...\")\n",
    "\n",
    "# Define the range of components (clusters) to test\n",
    "n_components_range = range(1, 11) # Example: 1 to 10 components\n",
    "covariance_types = ['spherical', 'tied', 'diag', 'full'] # Common covariance types\n",
    "\n",
    "# We'll use 'full' covariance for this example, but you should test others\n",
    "# and select the best based on BIC/AIC or other criteria.\n",
    "# For a fixed number of components (e.g., 4, to match your K-means)\n",
    "n_clusters_gmm = 4 # Example, tune this parameter\n",
    "gmm = GaussianMixture(n_components=n_clusters_gmm, covariance_type='full', random_state=42)\n",
    "\n",
    "# Fit GMM\n",
    "# If using PCA-transformed data: gmm.fit(X_pca)\n",
    "gmm.fit(X_scaled)\n",
    "\n",
    "# Get cluster assignments\n",
    "gmm_labels = gmm.predict(X_scaled) # or X_pca\n",
    "# Get probabilities of belonging to each cluster\n",
    "gmm_probs = gmm.predict_proba(X_scaled) # or X_pca\n",
    "\n",
    "print(f\"\\nCluster labels for the first 10 data points: {gmm_labels[:10]}\")\n",
    "# print(f\"Probabilities for the first data point: {gmm_probs[0]}\")\n",
    "print(f\"Number of clusters found: {len(np.unique(gmm_labels))}\")\n",
    "\n",
    "# To find the optimal number of components, you can iterate and check BIC/AIC\n",
    "bics = []\n",
    "aics = []\n",
    "\n",
    "print(\"\\nCalculating BIC and AIC for different numbers of components...\")\n",
    "for n_components in n_components_range:\n",
    "    gmm_iter = GaussianMixture(n_components=n_components, covariance_type='full', random_state=42)\n",
    "    gmm_iter.fit(X_scaled) # or X_pca\n",
    "    bics.append(gmm_iter.bic(X_scaled)) # or X_pca\n",
    "    aics.append(gmm_iter.aic(X_scaled)) # or X_pca\n",
    "\n",
    "# Plot BIC and AIC\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(n_components_range, bics, marker='o', label='BIC')\n",
    "plt.plot(n_components_range, aics, marker='o', label='AIC')\n",
    "plt.title('GMM: BIC and AIC for Number of Components')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Information Criterion Value (Lower is Better)')\n",
    "plt.xticks(n_components_range)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# The optimal number of components is often where BIC/AIC is minimized.\n",
    "# --- End of GMM ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4170599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors # For helping to choose eps\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "print(\"Starting DBSCAN...\")\n",
    "\n",
    "\n",
    "min_samples_dbscan = 10 # Tune this parameter\n",
    "\n",
    "print(f\"Calculating k-distances for eps estimation (min_samples = {min_samples_dbscan})...\")\n",
    "\n",
    "neigh = NearestNeighbors(n_neighbors=min_samples_dbscan)\n",
    "nbrs = neigh.fit(X_scaled) # or X_pca\n",
    "distances, indices = nbrs.kneighbors(X_scaled) # or X_pca\n",
    "\n",
    "# Sort the distances to the k-th nearest neighbor\n",
    "k_distances = np.sort(distances[:, min_samples_dbscan-1], axis=0)\n",
    "\n",
    "# Plot the k-distance graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_distances)\n",
    "plt.title(f'K-distance Graph (k={min_samples_dbscan}) for DBSCAN eps Estimation')\n",
    "plt.xlabel('Points sorted by distance to k-th nearest neighbor')\n",
    "plt.ylabel(f'{min_samples_dbscan}-th Nearest Neighbor Distance (eps candidate)')\n",
    "plt.grid(True)\n",
    "# Look for the \"elbow\" or \"knee\" in this plot to choose eps.\n",
    "# This point indicates a region where distances start to increase sharply.\n",
    "plt.show()\n",
    "print(\"Review the K-distance plot above to choose a suitable 'eps' value (y-axis value at the 'elbow').\")\n",
    "\n",
    "\n",
    "# --- DBSCAN Clustering ---\n",
    "# You need to set 'eps' based on the k-distance plot.\n",
    "# Let's pick an example value for eps; YOU MUST TUNE THIS.\n",
    "eps_dbscan = 0.5 # Example value, TUNE THIS CAREFULLY based on the plot above.\n",
    "\n",
    "print(f\"\\nRunning DBSCAN with eps={eps_dbscan} and min_samples={min_samples_dbscan}...\")\n",
    "dbscan = DBSCAN(eps=eps_dbscan, min_samples=min_samples_dbscan)\n",
    "\n",
    "# Fit DBSCAN\n",
    "# If using PCA-transformed data: dbscan.fit(X_pca)\n",
    "dbscan_labels = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "# Noise points are labeled as -1.\n",
    "n_clusters_dbscan = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "n_noise_dbscan = list(dbscan_labels).count(-1)\n",
    "\n",
    "print(f\"\\nEstimated number of clusters: {n_clusters_dbscan}\")\n",
    "print(f\"Estimated number of noise points: {n_noise_dbscan}\")\n",
    "# print(f\"Cluster labels: {np.unique(dbscan_labels)}\") # Shows all unique labels including -1 for noise\n",
    "\n",
    "# --- End of DBSCAN ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7029b8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fca7e4c",
   "metadata": {},
   "source": [
    "## Business Question\n",
    "\n",
    "**Can we predict the systematic risk (Beta) of mortgages in the FHFA NSMO portfolio using mortgage characteristics and macroeconomic factors?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7986a2",
   "metadata": {},
   "source": [
    "### Supervised Learning Questions\n",
    "\n",
    "- **Which mortgage features (loan characteristics, borrower profiles, property details) are most predictive of mortgage Beta?**\n",
    "- **How accurately can different ML models predict mortgage Beta?**\n",
    "- **Which model family (probabilistic, tree-based, instance-based) performs best for this financial prediction task?**\n",
    "\n",
    "### Unsupervised Learning Questions\n",
    "\n",
    "- **Are there natural groupings/clusters of mortgages based on their characteristics?**\n",
    "- **Do these clusters correspond to different risk profiles (Beta levels)?**\n",
    "- **Can we identify mortgage segments that behave similarly in terms of systematic risk?**\n",
    "\n",
    "### Financial/Business Questions\n",
    "\n",
    "- **How do mortgage characteristics influence systematic risk relative to market movements?**\n",
    "- **Can we identify high-Beta vs low-Beta mortgage profiles for portfolio management?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdd7c4a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mads_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
