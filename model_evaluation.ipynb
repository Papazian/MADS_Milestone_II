{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9c90906",
   "metadata": {},
   "source": [
    "# Read in the split data and load the YAML metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6088f22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required modules\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error\n",
    "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
    "from sklearn.metrics.pairwise import pairwise_distances_argmin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6839caaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40433, 879)\n",
      "(40433, 1)\n",
      "(10109, 879)\n",
      "(10109, 1)\n",
      "    x74r   term    ltv   cltv    dti    pti  age_o1  age_o2  age_o3  \\\n",
      "0 -1.898  0.560  0.919  0.905  0.249  1.233   0.000   0.000   0.000   \n",
      "1  1.294 -1.710  0.068  0.054 -0.628 -0.296   0.000   0.000   0.000   \n",
      "2  0.230  0.560  0.268  0.255 -0.788 -1.730   0.000   0.000   0.000   \n",
      "3  1.648  0.560 -1.235 -1.247  0.249  0.659   0.000   0.000   0.000   \n",
      "4  1.010  0.560 -0.734 -0.746 -0.788 -1.156   0.000   0.000   0.000   \n",
      "\n",
      "   score_orig_r  ...  close_month_5  close_month_6  close_month_7  \\\n",
      "0         0.144  ...          False          False          False   \n",
      "1         1.056  ...          False          False          False   \n",
      "2         0.576  ...          False          False          False   \n",
      "3        -0.912  ...          False          False          False   \n",
      "4         0.640  ...          False          False          False   \n",
      "\n",
      "   close_month_8  close_month_9  close_month_10  close_month_11  \\\n",
      "0          False          False           False            True   \n",
      "1          False          False           False           False   \n",
      "2          False          False           False           False   \n",
      "3          False          False           False           False   \n",
      "4          False          False           False           False   \n",
      "\n",
      "   close_month_12  cashout_1  cashout_2  \n",
      "0           False      False       True  \n",
      "1           False      False       True  \n",
      "2           False      False       True  \n",
      "3           False       True      False  \n",
      "4           False      False       True  \n",
      "\n",
      "[5 rows x 879 columns]\n",
      "   Beta_winsorized\n",
      "0            1.009\n",
      "1            1.774\n",
      "2            1.438\n",
      "3            1.248\n",
      "4            0.988\n",
      "    x74r  term   ltv  cltv    dti    pti  age_o1  age_o2  age_o3  \\\n",
      "0 -1.756 0.560 1.019 1.005  0.408  0.086   0.000   0.000   0.000   \n",
      "1 -0.763 0.560 0.268 0.255 -0.867  0.277   0.000   0.000   0.000   \n",
      "2 -0.267 0.560 1.069 1.055  0.727  0.373   0.000   0.000   0.000   \n",
      "3  0.655 0.560 0.268 0.255 -0.788 -0.870   0.000   0.000   0.000   \n",
      "4 -0.550 0.560 0.919 0.905 -0.230 -0.392   0.000   0.000   0.000   \n",
      "\n",
      "   score_orig_r  ...  close_month_5  close_month_6  close_month_7  \\\n",
      "0        -1.056  ...          False          False          False   \n",
      "1         1.248  ...          False          False          False   \n",
      "2        -1.344  ...          False          False          False   \n",
      "3        -0.464  ...          False          False          False   \n",
      "4        -0.912  ...          False          False          False   \n",
      "\n",
      "   close_month_8  close_month_9  close_month_10  close_month_11  \\\n",
      "0          False          False           False           False   \n",
      "1          False          False           False           False   \n",
      "2          False          False            True           False   \n",
      "3           True          False           False           False   \n",
      "4          False          False           False           False   \n",
      "\n",
      "   close_month_12  cashout_1  cashout_2  \n",
      "0           False      False       True  \n",
      "1           False      False       True  \n",
      "2           False      False       True  \n",
      "3           False      False       True  \n",
      "4           False      False       True  \n",
      "\n",
      "[5 rows x 879 columns]\n",
      "   Beta_winsorized\n",
      "0            1.224\n",
      "1            1.126\n",
      "2            1.220\n",
      "3            0.898\n",
      "4            1.621\n"
     ]
    }
   ],
   "source": [
    "# Import the split data\n",
    "X_train_path = 'bin/X_train.csv'\n",
    "y_train_path = 'bin/y_train.csv'\n",
    "X_test_path = 'bin/X_test.csv'\n",
    "y_test_path = 'bin/y_test.csv'\n",
    "\n",
    "X_train = pd.read_csv(X_train_path)\n",
    "y_train = pd.read_csv(y_train_path)\n",
    "X_test = pd.read_csv(X_test_path)\n",
    "y_test = pd.read_csv(y_test_path)\n",
    "\n",
    "# print the shape of the data\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# print the first 5 rows of the data\n",
    "print(X_train.head())\n",
    "print(y_train.head())\n",
    "print(X_test.head())\n",
    "print(y_test.head())\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3174df18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   1.009\n",
      "1   1.774\n",
      "2   1.438\n",
      "3   1.248\n",
      "4   0.988\n",
      "Name: Beta_winsorized, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Set the target variable\n",
    "target_variable = 'Beta_winsorized'\n",
    "\n",
    "# print the value of the target variable from the training data to validate it loaded correctly\n",
    "print(y_train[target_variable].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4724dac6",
   "metadata": {},
   "source": [
    "# Supervised Learning Models: Gradient Boosting, Linear, LASSO, and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b5c8426",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pdsmbp/miniconda3/envs/mads_env/lib/python3.10/site-packages/sklearn/utils/validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Fit a Gradient Boosting Regression model on the training partition, and then evaluate it on the testing partition\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html\n",
    "\n",
    "params = {'max_depth': 4, 'learning_rate': 0.01, 'random_state':0}\n",
    "grad_boost_reg = HistGradientBoostingRegressor(**params)\n",
    "grad_boost_reg.fit(X_train, y_train)\n",
    "\n",
    "mae = mean_absolute_error(y_test, grad_boost_reg.predict(X_test))\n",
    "mse = mean_squared_error(y_test, grad_boost_reg.predict(X_test))\n",
    "rmse = np.sqrt(mse)\n",
    "grad_boost_reg_dict = {\"model\": \"Gradient Boosting Regression\", \n",
    "                       \"Mean Absolute Error\": mae, \n",
    "                       \"Root Mean Squared Error\": rmse}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74b642fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a Linear Model on the training partition, and evaluate it on the testing partition\n",
    "# There is a lot of Multicollinearity in this model because we are putting all predictor variables into the model\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "\n",
    "linear_reg = linear_model.LinearRegression()\n",
    "linear_reg.fit(X_train,y_train)\n",
    "\n",
    "mae = mean_absolute_error(y_test, linear_reg.predict(X_test))\n",
    "mse = mean_squared_error(y_test, linear_reg.predict(X_test))\n",
    "rmse = np.sqrt(mse)\n",
    "linear_reg_dict = {\"model\": \"Linear Regression\", \n",
    "                   \"Mean Absolute Error\": mae, \n",
    "                   \"Root Mean Squared Error\": rmse}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10b7b290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a LASSO regression on the training partition, and then evaluate it on the testing partition\n",
    "# This will shrink most of the variable coefficients to zero for automated variable selection\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso\n",
    "# Default hyperparameters: alpha=1.0, max_iter=1000, tol=0.0001\n",
    "\n",
    "linear_reg_with_lasso = linear_model.Lasso(alpha=1.0, max_iter=1000, tol=0.0001)\n",
    "linear_reg_with_lasso.fit(X_train,y_train)\n",
    "\n",
    "mae = mean_absolute_error(y_test, linear_reg_with_lasso.predict(X_test))\n",
    "mse = mean_squared_error(y_test, linear_reg_with_lasso.predict(X_test))\n",
    "rmse = np.sqrt(mse)\n",
    "lasso_reg_dict = {\"model\": \"LASSO Regression\", \n",
    "                   \"Mean Absolute Error\": mae, \n",
    "                   \"Root Mean Squared Error\": rmse}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "646db486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the LASSO regression to identify the most useful variable to predict the target (i.e., the variables whose coefficient didn't shrink to zero)\n",
    "\n",
    "variable_names = list(X_train.columns) \n",
    "linear_reg_with_lasso_coef = linear_reg_with_lasso.coef_\n",
    "\n",
    "for variable, coef in zip(variable_names, linear_reg_with_lasso_coef):\n",
    "    # if the coefficient didn't shrink to zero\n",
    "    if (coef != 0):\n",
    "        # if it's a categorical variable\n",
    "        if variable in new_categorical_variables:\n",
    "            variable_substrings = variable.split(\"_\")  # split up the new categorical variable name by underscores\n",
    "            categorical_variable = '_'.join(variable_substrings[:-1])  # retrieve the original categorical variable name (e.g., 'x05a', 'perf_status_0923', etc.)\n",
    "            category = variable_substrings[-1:][0]  # retrieve the category chosen for the categorical variable (e.g., '1', '10', '2013', 'A', etc.)\n",
    "            if any(character.isdigit() for character in category):\n",
    "                category = int(category)\n",
    "            else:\n",
    "                category = str(category)\n",
    "            print(variable, \":\", variable_labels_dict[categorical_variable], \":\", categorical_variables_categories_dict[variable_formats_dict[categorical_variable]][category], \":\", coef)\n",
    "        # else it's a numeric variable\n",
    "        else:\n",
    "            print(variable, \":\", variable_labels_dict[variable], \":\", coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30724eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pdsmbp/miniconda3/envs/mads_env/lib/python3.10/site-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Fit a Random Forest regression model on the training partition, and then evaluate it on the testing partition\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "\n",
    "params = {'n_estimators': 100, 'max_depth': 4, 'random_state':0}\n",
    "random_forest_reg = RandomForestRegressor(**params)\n",
    "random_forest_reg.fit(X_train, y_train)\n",
    "\n",
    "mae = mean_absolute_error(y_test, random_forest_reg.predict(X_test))\n",
    "mse = mean_squared_error(y_test, random_forest_reg.predict(X_test))\n",
    "rmse = np.sqrt(mse)\n",
    "random_forest_reg_dict = {\"model\": \"Random Forest Regression\", \n",
    "                          \"Mean Absolute Error\": mae, \n",
    "                          \"Root Mean Squared Error\": rmse}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6eb9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Random Forest regression model to identify the top N most important feature based upon their respective feature importance values\n",
    "\n",
    "top_N = 20\n",
    "feature_importances = pd.Series(data=random_forest_reg.feature_importances_, index=X_train.columns)\n",
    "feature_importances.sort_values(ascending=False, inplace=True)\n",
    "top_N_features = feature_importances.head(top_N)\n",
    "\n",
    "# Print the top N feature by importance from Random Forest regression model in descending order\n",
    "for variable in top_N_features.index:\n",
    "    feature_importance = top_N_features[variable]\n",
    "    # if the feature importance is greater than zero\n",
    "    if (feature_importance > 0):\n",
    "        # if it's a categorical variable\n",
    "        if variable in new_categorical_variables:\n",
    "            variable_substrings = variable.split(\"_\")  # split up the new categorical variable name by underscores\n",
    "            categorical_variable = '_'.join(variable_substrings[:-1])  # retrieve the original categorical variable name (e.g., 'x05a', 'perf_status_0923', etc.)\n",
    "            category = variable_substrings[-1:][0]  # retrieve the category chosen for the categorical variable (e.g., '1', '10', '2013', 'A', etc.)\n",
    "            if any(character.isdigit() for character in category):\n",
    "                category = int(category)\n",
    "            else:\n",
    "                category = str(category)\n",
    "            print(variable, \":\", variable_labels_dict[categorical_variable], \":\", categorical_variables_categories_dict[variable_formats_dict[categorical_variable]][category], \":\", feature_importance)\n",
    "        # else it's a numeric variable\n",
    "        else:\n",
    "            print(variable, \":\", variable_labels_dict[variable], \":\", feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa8bf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the performance metrics for the various models on the holdout testing data\n",
    "\n",
    "all_model_performance_metrics = [grad_boost_reg_dict,\n",
    "                                linear_reg_dict,\n",
    "                                lasso_reg_dict,\n",
    "                                random_forest_reg_dict]\n",
    "\n",
    "all_model_performance_metrics_df = pd.DataFrame(all_model_performance_metrics)\n",
    "print(all_model_performance_metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8574cb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Validation Setup\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "# Set up 5-fold cross-validation\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "print(\"Starting Gradient Boosting hyperparameter tuning...\")\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "\n",
    "# Hyperparameter tuning for Gradient Boosting\n",
    "# Reduced parameter grid to avoid memory issues\n",
    "gb_param_grid = {\n",
    "    'max_depth': [10, 15, 20],\n",
    "    'learning_rate': [.02, .03, .04],\n",
    "    'max_iter': [500, 1000, 1500]\n",
    "}\n",
    "\n",
    "total_fits = len(gb_param_grid['max_depth']) * len(gb_param_grid['learning_rate']) * len(gb_param_grid['max_iter']) * 5  # 5-fold CV\n",
    "print(f\"Total model fits to perform: {total_fits}\")\n",
    "\n",
    "# Use verbose=2 to see progress, n_jobs=2 to avoid memory issues\n",
    "gb_grid = GridSearchCV(\n",
    "    HistGradientBoostingRegressor(random_state=0),\n",
    "    gb_param_grid,\n",
    "    cv=cv,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_jobs=2,  \n",
    "    verbose=2  # Shows progress\n",
    ")\n",
    "\n",
    "print(\"\\nFitting models...\")\n",
    "gb_grid.fit(X_train, y_train)\n",
    "grad_boost_reg = gb_grid.best_estimator_\n",
    "\n",
    "print(f\"\\nBest parameters found: {gb_grid.best_params_}\")\n",
    "print(f\"Best CV score: {-gb_grid.best_score_:.3f}\")\n",
    "\n",
    "# Evaluate with cross-validation on best model\n",
    "print(\"\\nPerforming cross-validation on best model...\")\n",
    "cv_scores = cross_val_score(grad_boost_reg, X_train, y_train, cv=cv, \n",
    "                           scoring='neg_mean_absolute_error', verbose=1)\n",
    "print(f\"CV MAE: {-cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "\n",
    "# Test set evaluation\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "mae = mean_absolute_error(y_test, grad_boost_reg.predict(X_test))\n",
    "rmse = np.sqrt(mean_squared_error(y_test, grad_boost_reg.predict(X_test)))\n",
    "\n",
    "print(f\"Test MAE: {mae:.3f}\")\n",
    "print(f\"Test RMSE: {rmse:.3f}\")\n",
    "\n",
    "grad_boost_cv_dict = {\n",
    "    \"model\": \"Gradient Boosting Regression (Cross-Validation)\", \n",
    "    \"Mean Absolute Error\": mae, \n",
    "    \"Root Mean Squared Error\": rmse,\n",
    "    \"Best Parameters\": gb_grid.best_params_\n",
    "}\n",
    "print(\"\\nGradient Boosting tuning complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6c0104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the performance metrics for the various models on the holdout testing data\n",
    "\n",
    "all_model_performance_metrics = [grad_boost_cv_dict,\n",
    "                                grad_boost_reg_dict,    \n",
    "                                linear_reg_dict,\n",
    "                                lasso_reg_dict,\n",
    "                                random_forest_reg_dict,]\n",
    "\n",
    "all_model_performance_metrics_df = pd.DataFrame(all_model_performance_metrics)\n",
    "print(all_model_performance_metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d54a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grad_boost_cv_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mads_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
