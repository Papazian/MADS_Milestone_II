Milestone II (SIADS 696) Project Guidelines Version 2025.4.14IntroductionTechnology ChoicesDraft Proposal GuidelinesPeer Review GuidelinesFinal Proposal GuidelinesProject Report GuidelinesIntroductionThe project component of Milestone II is designed to provide a comprehensive synthesis and assessment of the core data analytics skills that you’ve gained so far, including, for example, an end-to-end machine learning process applied to a challenging prediction problem. It’s an opportunity to pursue all the steps in a typical end-to-end solution path for an interesting problem, including: analysis, implementation, evaluation, and communication of the problem and results, applied to more advanced/specialized problems similar to those you may encounter later in the degree, and in your career. Project CoachYou will be assigned a “project coach” from among the lead instructional faculty for this course. Your project coach will follow your project throughout its lifecycle in this course: from initial project proposal, to written project feedback, to the eventual grading of the written report at the end of the course1. You are also required to have a discussion about the state of your project at least twice (by designated times specified in the syllabus) with your project coach. These meetings will happen asynchronously via DM in Slack, as well as synchronously via teams signing up for project meeting slots their coach makes available.StandupsAs part of the course, you and your team will do 2 standup video meetings, as well as respond to other groups’ standups. See the course syllabus for the dates on which these should happen. These will take place in a separate channel set up for the purpose. For more information on standups and what we’re looking for in them, see this write-up on “How to do a Standup”.Dataset For this project, you may choose your own dataset. You may use the same dataset for both the supervised and unsupervised portions of the project (see Methodology below); you may also use different datasets for each portion.Conditions for all datasets. Any dataset you use must adhere to the following conditions, or your project may not be approved.? The dataset and methods you use must not have a readily available solution/tutorial on the internet. For example, you cannot use a dataset (and associated methods) that has been the subject of an existing public Kaggle competition or tutorial, or one that has a blog post with a corresponding full git repo comprising the implementation. If you’re interested in a dataset that’s already been widely used in a public machine learning competition, consider that it may be possible to create a completely new dataset and prediction problem by joining/manipulating two existing datasets—if you want to do this, please talk w/ the instructors. It also may be possible to use a dataset that has been the subject of much exploration online, but then analyze it in a new way.? The dataset (and code) must be freely accessible to the instructors:, i.e. it cannot require a proprietary environment to access, or have any legal restrictions such as requiring non-disclosure, data use, or IP agreements. One possible workaround (if appropriate) would be to export a version that hides the meaning of the features (assuming there is no personally identifiable or other sensitive information in the dataset itself).? The dataset must offer sufficient challenge, in the judgment of the instructors, e.g. in terms of the number of data instances (rows) and the nature/complexity of variables (columns).? The instructors will evaluate these conditions when they review your project proposals.MethodologyThe project must incorporate both supervised and unsupervised learning components. You may use the same dataset for both parts or you may select different datasets for each. A. Supervised learning component. Using your selected dataset, you will endeavor to use several different learning frameworks and feature representations. The goal will be to showcase your understanding of various supervised learning approaches including comparison and evaluation of model results. B. Unsupervised learning. The goal here is to cluster the items in the dataset, or otherwise find an interesting structure, without labeled data, and to evaluate and describe your outcomes. Building on a Milestone I (or other previous course) ProjectIf you want to propose that your Milestone II project be a continuation of your Milestone I project (or any other course project), it must satisfy the dataset conditions above, and assuming it does, in your project proposal you must also provide a clear breakdown of the previous existing code you’re re-using, compared to the new code you propose to create. We expect re-use to include code for data manipulation or integration: you cannot re-use any supervised or unsupervised learning code for which you got credit in Milestone I (or another course), as part of your Milestone II project. Failure to disclose such re-use may be considered academic misconduct.Generative AI PolicyYour project’s written report is the main way for you to communicate to us about your decision-making process and interpretation of your results. For this reason, the use of GenAI, such as ChatGPT, is prohibited for the written report. If these technologies are used for assistance in the coding portion of the project, the use of GenAI must be acknowledged or cited in the documentation and written report. As stated in the syllabus, students who are found to have used ChatGPT or the like outside of these requirements will be reported to UMSI Student Affairs for investigation as academic misconduct, and subject to consequences like failing the assignment or failing the course depending on the scope and severity of the actions taken.Project TimelineThe Milestone II project timeline will generally follow a similar one to that of Milestone I:? Team formation and pre-proposal (via Google Sheets)? Draft proposal (via Google Docs)? Peer review of proposals (commenting on Google Docs)? Asynchronous discussions with project coach (see above)? Instructor-reviewed proposals (via Coursera)? Synchronous discussion with project coach (see above)? Final report (via Coursera and Google Docs)Team formation: For Milestone II, we require by default that you work in teams of three. Although this recommendation can be relaxed to permit individual or two-person projects in exceptional circumstances, it has been our experience that working in teams balances high-quality work with the amount of work each person needs to do to complete the project. An example of one of the few scenarios where a one-person project would be allowed would be if that project required highly specialized domain expertise that would be problematic for other team members to learn quickly.Project teams will receive one grade for submitted projects, which will be assigned to each member of the team. Grades are not assigned on an individual basis for team projects.The rubric used to grade this Milestone project is the same regardless of team size: all teams must respond to the rubric’s requirements as described here and have all required sections in their final report. Overall, the goal of the Milestone II project work is to provide an opportunity to really bring together the diverse skills required of a data scientist, on problems that demonstrate your ability to apply the skills and concepts that you learned in the previous courses, while also being able to work in a domain and real dataset of your choice. In any case, for this project work you must make significant use of programming tools covered in prior MADS courses. Project Proposal: Before starting work on the actual project, you'll write a short project proposal (see guidelines below) that is meant to be a high-level summary and does not need to contain technical details or code. You will discuss possible approaches to Part A (supervised learning) and Part B (unsupervised learning). You will then review your peers' proposals, and they will review yours. You can adjust your proposal based on input you receive and then you'll submit it to the course instructors for grading and approval. Requiring the proposal to be submitted early is intended to get you thinking about questions and datasets you're interested in, and how those might be answered with the tools you've been exposed to in previous classes.Computing ChoicesThis course differs from your other MADS courses in many ways including technology. You have a number of choices described below. Do not wait until the last minute to set up the computing environment you’ll need!  You should have the computing environment you’ll need for intensive training and evaluation ready no later than week 5, and preferably earlier, especially if you have special needs. If in doubt, ask the instructional team!Additionally, you will be required to submit all of your code files with your written report stored in a GitHub repository. Additional assistance getting started with GitHub will be provided as needed.1. We have created a Jupyter environment for you that is functionally equivalent to SIADS 516, which is a superset of the base MADS environment, and you can access that environment via the "ungraded lab assignment" in Coursera. This is a Coursera “big instance” with 4 CPU and 16 Gb RAM. For file storage, you have a 1Gb limit as the max size for a single file, and you have 10Gb of total space.  Use this environment if you have basic needs. If you plan to use deep learning that requires a GPU (which it usually does), then consider using one of the other options below, since we’ve had mixed results with the GPU support in Coursera Labs.You can use this Coursera environment or choose to use any of the environments from courses you have already completed to build and test data manipulations and visualizations for your project. You can also use your own locally installed environment. 2. We encourage you to explore the use of cloud computing in your milestone project: you’ll find it useful even if you’re not doing deep learning, because the hyperparameter tuning and evaluation processes required by the report can also be computationally intensive. Also, it’s good just to get experience using a new platform. We strongly recommend making use of UMich cloud computing using the Great Lakes computing environment. This is especially true if you have very intensive computing needs (e.g. GPU for deep learning). Great Lakes is the University of Michigan's campus-wide computing cluster provided by ARC (Advanced Research Computing), a division of ITS (Information Technology Services). Great Lakes | ITS Advanced Research Computing. It is the shared, Linux-based high-performance computing (HPC) cluster available to researchers at the University of Michigan. Great Lakes consists of approximately 13,000 cores – including compute nodes composed of multiple CPU cores, GPUs, large-memory nodes, and support for simulation, modeling, machine learning, data science, genomics, and more.All project based MADS courses offer this computing resource for students/teams with high end computing needs for their projects. Many previous Milestone 2 teams have used Great Lakes successfully. It is easy to set up, and you don’t need to be an expert in Linux - in fact, there is a very straightforward Jupyter notebook mode!  Here’s the link to the student guide:Great_Lakes_Student Instructions_Milestone 2 Summer 20253. Another possibility is to use Google Colaboratory or DeepNote, which may facilitate collaboration if needed. The School will not pay the access/subscription fees for these services.Whatever computing environment you choose to utilize while working on your project, the files must ultimately be submitted by providing a link to a GitHub repository.Draft Proposal GuidelinesYou should use the following outline for your proposal, which shouldn't need more than about one page. You should address each of the points below, with a few sentences each:? Provide an introduction/overview that speaks to the goals of the project, as well as any challenges or limitations you foresee in the approach and/or dataset.? Find the closest example you can of one existing project or study that is most similar to what you’re proposing: does it use the same dataset or address the same prediction task?  Include the link, a few sentences of description, and summarize how what you’re proposing is different or improved compared to that existing work. This includes the case where the proposed project is a continuation of a previous milestone or course project for which you received credit: you must state that in the proposal and summarize old vs new work. You cannot propose to reuse (i.e. get credit twice for) a previous project’s supervised or unsupervised learning work to satisfy the Milestone 2 project requirements. When in doubt, please ask the instructors.Part A (Supervised learning)? What is the dataset you propose to use? Provide a link to the dataset.? Specify the learning approaches and feature representations that are appropriate to this problem: what do you plan to try, and why? (Your choices could change later.)? Are there external datasets or tools that you might incorporate to help with the problem? If so, what are they and what additional work is required to prepare/use them?? Describe the evaluation and visualization methods you plan to use.Part B (Unsupervised learning)? What is the dataset you propose to use? Provide a link to the dataset.? What are the question(s) about the dataset’s structure you want to answer, or goal to achieve?? What data manipulation will be necessary for this dataset to prepare it?? Specify unsupervised learning approaches and feature representations that are appropriate for this problem.? Are there external datasets or tools that you might incorporate to help with the problem?? How will you evaluate the quality of your results?? Describe visualizations that would be appropriate as part of evaluating the effectiveness of your methods or characterizing the structure in the dataset.Team Planning:? Indicate the specific contributions that each team member will make to the project.? Include a rough timeline.Your proposals will be reviewed by two peers from the class and you will take those into consideration when you revise your proposal for review by the instructional team. You will also discuss your proposal directly with your project coach. Your draft proposal should be a Google Doc that you share. Please note that you must enable commenting on your Google Doc to receive peer reviews.Peer Review GuidelinesYou will receive credit for completing reviews of your peers' proposals (see syllabus for grading). Note that your proposal grade will not be based on the content of your peers' reviews. The purpose of the peer reviews is threefold: (1) to gain experience reviewing proposals, (2) to learn about other work going on in class, and (3) to get feedback on how to make your project better. Your reviews should be 5-10 sentences long and should take into consideration the following points:? professional: what would a co-worker think about your review?? pleasant: courtesy goes a long way.? helpful: what sort of advice would you want?? scientific: focus on facts, not opinions.? realistic: keep scope in mind.? empathetic: how would you feel if you received the review you wrote?? organized: make it easy for the recipient to follow your train of thought.In machine learning, both positive and negative examples are important: the same is true for peer reviews! Thus, it’s important to highlight positive things in peer reviews, as well as areas for improvement. Typically you should point to at least two positive things that the authors did well and identify one area where they might spend some time improving their work (always making specific, constructive suggestions about how to do so).Peer reviews will take the form of comments on the Google Doc containing the proposal and will be coordinated via a Google Sheet.Per the instructions in the Coursera assignment description, you are expected to complete two (2) peer reviews.Final Proposal GuidelinesTaking into consideration the feedback from your peers' reviews of your draft proposal, you will revise your proposal and submit it to the course instructors for final proposal review. The course instructors may make suggestions to your plans and will give the official "go-ahead" to proceed with your project if the final proposal is approved. (Otherwise, they will work with you to find an alternative or acceptable variant.)You will submit the proposal as a PDF document via Coursera. You will also provide a link to the Google Doc version of your report to your project coach.Project Report GuidelinesOriginal work policy. First and foremost, following the individual original work policy clearly stated at the start of the course, all of your team’s project work must be of your team’s own invention. If any team members used any content at all from a particular web site or previous project, or worked on the project as part of a previous milestone, course, or existing research collaboration, you must identify all your sources and/or collaborators and provide links and citation(s) to all material that contributed to your project. This includes cases where you modified or re-used a portion of a notebook or source code provided by someone else. The instructional staff reserves the right to review all submitted work for potential plagiarism. Claiming any work of others as your own, either by direct misattribution or failure to attribute, is grounds for academic misconduct charges. If in doubt, please contact the instructional team.Report formatting. Here are the formatting rules you must follow for your report:? You must use Times New Roman or Arial for your main font, no smaller than 10 point and no larger than 12 point font size.? Use black text on a white background. ? Single-spaced, single column is preferred. ? The report must have a title and include the names of all team members.? The main body of the final report will typically be at least 10 pages, but must be no more than 15 pages total. This 15 page limit includes all the main narrative text, tables, and figures. References do not count toward the page limit. Reports that clearly don’t respect this page limit are likely to be penalized.? If you want to include extra material (e.g. code, supplemental results) that's useful for more detail but not critical to your main narrative, you can put it into an appendix section, which will not count towards the page limit. However, similar to the supplemental material in a submitted conference paper it would be at the instructors’ discretion on whether to include the extra appendix content in their grading.? The report itself must be completely self-contained in a single PDF file. If there are plots in a notebook that you want to include, you need include them as figure in the main body of the report (if they are part of your main narrative) or put them in an appendix in the report (if they aren't required to satisfy the report requirements, but you want to show more details behind a result).? You can author your report using a Jupyter notebook as long as the exported PDF report looks exactly like a normal written report that adheres to all the requirements. The “SlideDoc” format using a presentation-like series of slides in a deck is not a format we’ll accept for this Milestone. ? Please number all tables and figures!Within the 15-page limit, you should be able to select the key visualizations or tables you want to highlight to create a complete narrative with representative results. Extra material, per the above description, can go into an appendix. Summarizing complex material effectively by combining text and graphics is an important professional skill, especially for highly technical work, which is why we’re asking you to get experience doing it with this report.Report structure. The structure of the report is semi-flexible - you can include additional information and make adjustments to the order to suit your narrative flow, but at a minimum it should have the following sections. For the initial sections of the report below (introduction, related work, etc.), if it makes sense to separate them by supervised vs unsupervised (e.g. you used separate datasets for supervised and unsupervised learning), feel free to split that content into the corresponding supervised and unsupervised sections as needed.Introduction (5 points)? What problem are you trying to solve? ? What impact will solving the problem have? ? What motivated you to work on it?? Summarize your project’s supervised and unsupervised methods using for the problem, and highlight any novel contributions compared to related projects.? What are your main findings for supervised and unsupervised learning?Related work (5 points). Find and reference at least three examples of existing projects or studies that are most similar to your project: include 1-2 sentence descriptions for each, and 1-2 sentences summarizing how what you’re proposing is different or improved compared to this existing work. This is also the place to mention if this project is an extension of a Milestone 1 or other previous course project.For the Data Source and Feature Engineering sections below, if you use two different datasets or different feature engineering for supervised and unsupervised learning, just describe each of those in the appropriate sections of your report and we will merge the grading into the appropriate rubric sections.Data Source(s) (5 points): Describe the properties of the dataset (or data API service) you used. Be specific. Your information at a minimum should include but not be limited to:? where the datasets or API resource is located,? what formats they returned/used,? what were the important variables contained in them,? how many records you used or retrieved (if using an API), and? what time periods they covered (if there is a time element)Feature engineering (8 points)? Describe the steps you used to get from the raw input data to the final features used w/ supervised and unsupervised ML methods.? Explain any initial preprocessing that was required to handle noisy or missing data.? Be sure to include a complete list of all final features (in an appendix if necessary).Part A. Supervised Learning? Methods description (8 points)? Briefly describe your supervised learning workflow, the learning methods you used, and the feature representations you chose. You must justify why you chose your methods.? You should have an adequate number and nature of methods: a minimum of three diverse model families must be described and explored (i.e. with very different underlying mechanisms, e.g. probabilistic, non-probabilistic, tree-based, instance-based, etc).  This might be variable with larger or smaller teams and project specifics, with instructor permission. ? Include a description of how you did hyperparameter tuning or exploration with your models.? Methods used must be clearly described, each with correct justification.? Supervised Evaluation (22 points)? In this section you will provide a correct and comprehensive evaluation, analyzing the effectiveness of both your methods, and your feature representations. ? (8 points) Overall results reporting. ? State and justify your choice of evaluation metrics used.? Provide at least one overall summary of results that compares the best model from each family you used, in a clear, concise table. ? If comparing an evaluation metric between model families (e.g. comparing accuracy of support vector machines vs logistic regression), do not use just the result of a single training/test split: you must report the mean metric across multiple cross-validation folds (typically 5-fold CV), along with the standard deviation of the metric.? Please see the “Tips for Project Report” video under “Week 6 Project Check-in” for key methods to get insight into your machine learning model and how to report results.? For the following parts of the evaluation, typically you will do these deeper analysis steps only on your best-performing model (not all of them).? (6 points) Do a feature importance and ablation analysis on your best model to get insight into which features are or are not contributing to prediction success/failure.? (4 points) Do at least one sensitivity analysis on your best model: How sensitive are your results to choice of (hyper-)parameters, features, or other varying solution elements?? (4 points) Given your evaluation results and metrics, what important tradeoffs can you identify?  Some examples:  precision vs recall, training data size vs accuracy, speed vs accuracy, etc.? Failure analysis (5 points)? Select at least 3 *specific* examples (records) where prediction failed, and analyze possible reasons why. ? Ideally you should be able to identify at least three different categories of failure.? What future improvements might fix the failures?  (You do not need to implement these)Part B. Unsupervised Learning ? Methods description (10 points)? Briefly describe your unsupervised learning workflow, the learning methods you used, and the feature representations you chose. You must justify why you chose your methods.? You should have an adequate number and nature of methods: a minimum of two unsupervised methods must be described and explored (i.e. with very different underlying mechanisms, e.g. probabilistic, non-probabilistic, tree-based, instance-based, etc).  This might be variable with larger or smaller teams and project specifics, with instructor permission. ? Include a description of how you did hyperparameter tuning or exploration with your models.? Methods used must be clearly described, each with correct justification.? Unsupervised Evaluation (15 points)? In this section you will provide a correct and comprehensive evaluation, analyzing the effectiveness of both your methods, and your choice of feature representation. ? (10 points) Overall results reporting. ? State and justify your choice of evaluation metrics used.? Provide at least one overall summary of results that compares the best model from each family you used, in a clear, concise table.? To summarize your findings, include at least two visualizations (chart, plot, tag cloud, map or other graphic) for each unsupervised method used that summarize your analysis.? (5 points) Do at least one sensitivity analysis on your best model: How sensitive are your results to choice of (hyper-)parameters, features, or other varying solution elements?Discussion (8 points)? (4 points) What did you learn from doing Part A? a. What surprised you about your results? b. What challenges did you encounter, and how did you respond to them?c. How could you extend your solution with more time/resources?? (4 points) What did you learn from doing Part B? a. What surprised you about your results? b. What challenges did you encounter, and how did you respond to them?c. How could you extend your solution with more time/resources?Ethical Considerations (4 points)a. What ethical issues could arise in providing a solution to Part A, and how could you address them? b. What ethical issues could arise in providing a solution to Part B, and how could you address them? Statement of Work (1 point)? You must include a statement that describes the contribution that each team member made to the project.References? You must include a reference section at the end of your report. Use standard citation style to include references to any papers, projects, web sites, or other resources you used for your project. References do not count toward the page limit.Project Submission (4 points)? (3 points) Link to GitHub repository with all project code, pdf file of written report, and data sources.? (1 point) Link to Google Doc version of written report with comments enabled.(See below for additional project submission guidelines.)At the discretion of the instructor up to 10 bonus points will be awarded for especially high-quality, creative or insightful projects.Instructor suggestions for creating an effective report1. Your report should not just describe your methods - it should justify your choice of them. This applies to important choices like the learning framework/choice of model, individual features or feature types, overall feature representation, and evaluation methods. For example, if you decide to use two families of prediction model that are similar, can you justify why? (Perhaps because you’re exploring a particular hypothesis or question.)2. Your report should ultimately give at least some insight into why particular approaches were successful or failed -- either by themselves or compared to each other. At a minimum, you should at least focus on your failure analysis to characterize the types of failures that future work might try to address.3. As you’re planning your remaining time, we recommend prioritizing insight into features ahead of insight into models - although of course ultimately it will depend on your particular project, and ideally you should include at least some analysis of model differences. If you want to argue that in your application, model choices contribute more to gains than feature choice, by all means do that...4. In the discussion section, include at least a few sentences that identify and discuss important potential tradeoffs. You don’t necessarily have to show evidence for them, just note that they exist and why.5. When discussing potential ethical issues, it’s not just a question of whether there may be particular privacy or ethical issues in the specific dataset or method: if that’s less of a concern, you should focus on the more general question of potential ethical issues around *deploying* a machine learning artifact for your task, i.e. the use of that supervised or unsupervised learning task in making policy/judicial/educational/health and public safety decisions, etc.Examples of Past ReportsWe’ve put one or more excellent Milestone 2 report examples from a recent iteration in the following folder (link here). No report is “perfect”: there are things to improve in these examples, but each example is a good guide to the level of presentation and technical quality we are expecting. However, do not copy or rely on the specific structure of these reports, since project report guidelines may have been adjusted since those were submitted. Always refer to the current project guidelines in creating your own report.Submitting the Final ReportPlease submit the following components:? Link to GitHub repository* containing:? Your project report as a single PDF document? All source code files/scripts (Python and any other code) used for your project? Project data in one of the following formats:? The actual datafiles (if 25 Mb or less)? Working URLs that point to the actual data/API resources you used (this can be provided in your GitHub README)? A sample file containing the first 100 records (if full dataset is unavailable or too large)*Your GitHub repository must either be public or you must be sure to invite your project coach so they can access your files.? A Google Doc link that points to the final report with comments enabled. This will be where you receive specific, concrete, written feedback from your assigned project coach at the conclusion of the class. Please, however, allow adequate time to receive this feedback, as instructors are grading multiple dense and complicated project reports.As part of the grading the teaching team may attempt to reproduce your results using your code and data, and you are expected to assist with this if we request it.1 It is possible that a handful of teams will switch project coaches mid-way through the course, if new instructional faculty are added to the class; all discussions and feedback will be handed off, and we will attempt to make the transition seamless for you.------------------------------------------------------------------------------------------------------------------------------------------------------